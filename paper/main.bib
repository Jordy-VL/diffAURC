% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@misc{SpaCyNER,
  title        = {{SpaCy} \texttt{en\_core\_web\_lg} Label Scheme},
  note         = {Accessed: 2023-03-08},
  howpublished = {\url{https://spacy.io/models/en#en_core_web_lg-labels}}
}
@article{simsa2023docile,
  author  = {{\v{S}}imsa, {\v{S}}t{\v{e}}p{\'a}n and {\v{S}}ulc, Milan and U{\v{r}}i{\v{c}}{\'a}{\v{r}}, Michal and Patel, Yash and Hamdi, Ahmed and Koci{\'a}n, Mat{\v{e}}j and Skalick{\`y}, Maty{\'a}{\v{s}} and Matas, Ji{\v{r}}{\'\i} and Doucet, Antoine and Coustaty, Micka{\"e}l and others},
  year    = 2023,
  title   = {DocILE Benchmark for Document Information Localization and Extraction},
  journal = {arXiv preprint arXiv:2302.05658}
}
@misc{rossum2022practicalbenchmarks,
  author    = {{Skalicky, Matyas and Simsa, Stepan and Uricar, Michal and Sulc, Milan}},
  year      = 2022,
  title     = {Business Document Information Extraction: Towards Practical Benchmarks},
  publisher = {arXiv},
  keywords  = {Information Retrieval (cs.IR), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@inproceedings{agashe-etal-2019-juice,
  author    = {Agashe, Rajas  and Iyer, Srinivasan  and Zettlemoyer, Luke},
  year      = 2019,
  month     = nov,
  title     = {{J}u{IC}e: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  publisher = {Association for Computational Linguistics},
  address   = {Hong Kong, China},
  pages     = {5436--5446},
  doi       = {10.18653/v1/D19-1546},
  url       = {https://aclanthology.org/D19-1546},
  abstract  = {Interactive programming with interleaved code snippet cells and natural language markdown is recently gaining popularity in the form of Jupyter notebooks, which accelerate prototyping and collaboration. To study code generation conditioned on a long context history, we present JuICe, a corpus of 1.5 million examples with a curated test set of 3.7K instances based on online programming assignments. Compared with existing contextual code generation datasets, JuICe provides refined human-curated data, open-domain code, and an order of magnitude more training data. Using JuICe, we train models for two tasks: (1) generation of the API call sequence in a code cell, and (2) full code cell generation, both conditioned on the NL-Code history up to a particular code cell. Experiments using current baseline code generation models show that both context and distant supervision aid in generation, and that the dataset is challenging for current systems.}
}
@misc{https://doi.org/10.48550/arxiv.1505.00468,
  author    = {Agrawal, Aishwarya and Lu, Jiasen and Antol, Stanislaw and Mitchell, Margaret and Zitnick, C. Lawrence and Batra, Dhruv and Parikh, Devi},
  year      = 2015,
  title     = {VQA: Visual Question Answering},
  publisher = {arXiv},
  doi       = {10.48550/ARXIV.1505.00468},
  url       = {https://arxiv.org/abs/1505.00468},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords  = {Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@misc{https://doi.org/10.48550/arxiv.1905.13319,
  author    = {Amini, Aida and Gabriel, Saadia and Lin, Peter and Koncel-Kedziorski, Rik and Choi, Yejin and Hajishirzi, Hannaneh},
  year      = 2019,
  title     = {MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms},
  publisher = {arXiv},
  doi       = {10.48550/ARXIV.1905.13319},
  url       = {https://arxiv.org/abs/1905.13319},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords  = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@inproceedings{mishraICDAR19,
  author    = {Anand Mishra and Shashank Shekhar and Ajeet Kumar Singh and Anirban Chakraborty},
  year      = 2019,
  title     = {OCR-VQA: Visual Question Answering by Reading Text in Images},
  booktitle = {ICDAR}
}
@inproceedings{kumar2019calibration,
  author    = {Ananya Kumar and Percy Liang and Tengyu Ma},
  year      = 2019,
  title     = {Verified Uncertainty Calibration},
  booktitle = {Advances in Neural Information Processing Systems}
}
@inproceedings{kumar2019calibration,
  author    = {Ananya Kumar and Percy Liang and Tengyu Ma},
  year      = 2019,
  title     = {Verified Uncertainty Calibration},
  booktitle = {Advances in Neural Information Processing Systems}
}
@incollection{Nentidis_2022,
  author    = {Anastasios Nentidis and Georgios Katsimpras and Eirini Vandorou and Anastasia Krithara and Antonio Miranda-Escalada and Luis Gasco and Martin Krallinger and Georgios Paliouras},
  year      = 2022,
  title     = {Overview of~{BioASQ} 2022: The Tenth {BioASQ} Challenge on~Large-Scale Biomedical Semantic Indexing and~Question Answering},
  booktitle = {Lecture Notes in Computer Science},
  publisher = {Springer International Publishing},
  pages     = {337--361},
  doi       = {10.1007/978-3-031-13643-6_22}
}
@inproceedings{antol2015vqa,
  author    = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
  year      = 2015,
  title     = {Vqa: Visual question answering},
  booktitle = {Proceedings of the IEEE international conference on computer vision},
  pages     = {2425--2433}
}
@inproceedings{appalaraju2021docformer,
  author    = {Appalaraju, Srikar and Jasani, Bhavan and Kota, Bhargava Urala and Xie, Yusheng and Manmatha, R},
  year      = 2021,
  title     = {Docformer: End-to-end transformer for document understanding},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages     = {993--1003}
}
% Early exits
@inproceedings{araujo2022entropy,
  author    = {Araujo, Vladimir and Hurtado, Julio and Soto, Alvaro and Moens, Marie-Francine},
  year      = 2022,
  title     = {Entropy-based Stability-Plasticity for Lifelong Learning},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {3721--3728}
}
@article{bakkali2023vlcdoc,
  author    = {Bakkali, Souhail and Ming, Zuheng and Coustaty, Mickael and Rusi{\~n}ol, Mar{\c{c}}al and Terrades, Oriol Ramos},
  year      = 2023,
  title     = {VLCDoC: Vision-Language contrastive pre-training model for cross-Modal document classification},
  journal   = {Pattern Recognition},
  publisher = {Elsevier},
  volume    = 139,
  pages     = 109419
}
@article{beltagy2020longformer,
  author  = {Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  year    = 2020,
  title   = {Longformer: The long-document transformer},
  journal = {arXiv preprint arXiv:2004.05150}
}
@inproceedings{bensch2021key,
  author    = {Bensch, Oliver and Popa, Mirela and Spille, Constantin},
  year      = 2021,
  title     = {Key Information Extraction From Documents: Evaluation And Generator},
  booktitle = {European Semantic Web Conference (ESWC 2021) and 2nd International Workshop, in conjunction with ESWC 2021: Workshop: Deep Learning meets Ontologies and Natural Language Processing}
}
@article{beyer2020we,
  author  = {Beyer, Lucas and H{\'e}naff, Olivier J and Kolesnikov, Alexander and Zhai, Xiaohua and Oord, A{\"a}ron van den},
  year    = 2020,
  title   = {Are we done with imagenet?},
  journal = {arXiv preprint arXiv:2006.07159}
}
@inproceedings{biswas2021docsynth,
  author       = {Biswas, Sanket and Riba, Pau and Llad{\'o}s, Josep and Pal, Umapada},
  year         = 2021,
  title        = {Docsynth: a layout guided approach for controllable document image synthesis},
  booktitle    = {International Conference on Document Analysis and Recognition},
  pages        = {555--568},
  organization = {Springer}
}
@article{biten2022ocr,
  author  = {Biten, Ali Furkan and Tito, Ruben and Gomez, Lluis and Valveny, Ernest and Karatzas, Dimosthenis},
  year    = 2022,
  title   = {Ocr-idl: Ocr annotations for industry document library dataset},
  journal = {arXiv preprint arXiv:2202.12985}
}
%% Competitions and Challenges
@inproceedings{biten2019icdar,
  author       = {Biten, Ali Furkan and Tito, Ruben and Mafla, Andres and Gomez, Lluis and Rusinol, Mar{\c{c}}al and Mathew, Minesh and Jawahar, CV and Valveny, Ernest and Karatzas, Dimosthenis},
  year         = 2019,
  title        = {Icdar 2019 competition on scene text visual question answering},
  booktitle    = {2019 International Conference on Document Analysis and Recognition (ICDAR)},
  pages        = {1563--1570},
  organization = {IEEE}
}
@inproceedings{biten2019scene,
  author    = {Biten, Ali Furkan and Tito, Ruben and Mafla, Andres and Gomez, Lluis and Rusinol, Mar{\c{c}}al and Valveny, Ernest and Jawahar, CV and Karatzas, Dimosthenis},
  year      = 2019,
  title     = {Scene text visual question answering},
  booktitle = {Proceedings of the IEEE/CVF international conference on computer vision}
}
@inproceedings{bjerva-etal-2020-subjqa,
  author    = {Bjerva, Johannes  and Bhutani, Nikita  and Golshan, Behzad  and Tan, Wang-Chiew  and Augenstein, Isabelle},
  year      = 2020,
  month     = nov,
  title     = {{SubjQA}: {A} {D}ataset for {S}ubjectivity and {R}eview {C}omprehension},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  publisher = {Association for Computational Linguistics},
  address   = {Online},
  pages     = {5480--5494},
  doi       = {10.18653/v1/2020.emnlp-main.442},
  url       = {https://aclanthology.org/2020.emnlp-main.442},
  abstract  = {Subjectivity is the expression of internal opinions or beliefs which cannot be objectively observed or verified, and has been shown to be important for sentiment analysis and word-sense disambiguation. Furthermore, subjectivity is an important aspect of user-generated data. In spite of this, subjectivity has not been investigated in contexts where such data is widespread, such as in question answering (QA). We develop a new dataset which allows us to investigate this relationship. We find that subjectivity is an important feature in the case of QA, albeit with more intricate interactions between subjectivity and QA performance than found in previous work on sentiment analysis. For instance, a subjective question may or may not be associated with a subjective answer. We release an English QA dataset (SubjQA) based on customer reviews, containing subjectivity annotations for questions and answer spans across 6 domains.}
}
@article{PAMI_places,
  author    = {Bolei Zhou and Agata Lapedriza and Aditya Khosla and Aude Oliva and Antonio Torralba},
  year      = 2018,
  month     = jun,
  title     = {Places: A 10 Million Image Database for Scene Recognition},
  journal   = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume    = 40,
  number    = 6,
  pages     = {1452--1464},
  doi       = {10.1109/tpami.2017.2723009},
  url       = {https://doi.org/10.1109/tpami.2017.2723009}
}
@inproceedings{borchmann2021due,
  author    = {Borchmann, {\L}ukasz and Pietruszka, Micha{\l} and Stanislawek, Tomasz and Jurkiewicz, Dawid and Turski, Micha{\l} and Szyndler, Karolina and Grali{\'n}ski, Filip},
  year      = 2021,
  title     = {DUE: End-to-End Document Understanding Benchmark},
  booktitle = {Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)}
}
%% calibration and selective classification
@book{bornet2021intelligent,
  author    = {Bornet, Pascal and Barkin, Ian and Wirtz, Jochen},
  year      = 2021,
  title     = {Intelligent automation: Welcome to the world of hyperautomation: learn how to harness artificial intelligence to boost business \& make our world more human},
  publisher = {World Scientific}
}
@article{brocker2009reliability,
  author    = {Br{\"o}cker, Jochen},
  year      = 2009,
  title     = {Reliability, sufficiency, and the decomposition of proper scores},
  journal   = {Quarterly Journal of the Royal Meteorological Society: A journal of the atmospheric sciences, applied meteorology and physical oceanography},
  publisher = {Wiley Online Library},
  volume    = 135,
  number    = 643,
  pages     = {1512--1519}
}
%% generic
@article{brown2020language,
  author  = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  year    = 2020,
  title   = {Language models are few-shot learners},
  journal = {Advances in neural information processing systems},
  volume  = 33,
  pages   = {1877--1901}
}
@inproceedings{castro-etal-2020-lifeqa,
  author    = {Castro, Santiago  and Azab, Mahmoud  and Stroud, Jonathan  and Noujaim, Cristina  and Wang, Ruoyao  and Deng, Jia  and Mihalcea, Rada},
  year      = 2020,
  month     = may,
  title     = {{L}ife{QA}: A Real-life Dataset for Video Question Answering},
  booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},
  publisher = {European Language Resources Association},
  address   = {Marseille, France},
  pages     = {4352--4358},
  isbn      = {979-10-95546-34-4},
  url       = {https://aclanthology.org/2020.lrec-1.536},
  abstract  = {We introduce LifeQA, a benchmark dataset for video question answering that focuses on day-to-day real-life situations. Current video question answering datasets consist of movies and TV shows. However, it is well-known that these visual domains are not representative of our day-to-day lives. Movies and TV shows, for example, benefit from professional camera movements, clean editing, crisp audio recordings, and scripted dialog between professional actors. While these domains provide a large amount of data for training models, their properties make them unsuitable for testing real-life question answering systems. Our dataset, by contrast, consists of video clips that represent only real-life scenarios. We collect 275 such video clips and over 2.3k multiple-choice questions. In this paper, we analyze the challenging but realistic aspects of LifeQA, and we apply several state-of-the-art video question answering models to provide benchmarks for future research. The full dataset is publicly available at https://lit.eecs.umich.edu/lifeqa/.},
  language  = {English}
}
@inproceedings{castro-etal-2022-wild,
  author    = {Castro, Santiago  and Deng, Naihao  and Huang, Pingxuan  and Burzo, Mihai  and Mihalcea, Rada},
  year      = 2022,
  month     = oct,
  title     = {In-the-Wild Video Question Answering},
  booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},
  publisher = {International Committee on Computational Linguistics},
  address   = {Gyeongju, Republic of Korea},
  pages     = {5613--5635},
  url       = {https://aclanthology.org/2022.coling-1.496},
  abstract  = {Existing video understanding datasets mostly focus on human interactions, with little attention being paid to the {``}in the wild{''} settings, where the videos are recorded outdoors. We propose WILDQA, a video understanding dataset of videos recorded in outside settings. In addition to video question answering (Video QA), we also introduce the new task of identifying visual support for a given question and answer (Video Evidence Selection). Through evaluations using a wide range of baseline models, we show that WILDQA poses new challenges to the vision and language research communities. The dataset is available at https: //lit.eecs.umich.edu/wildqa/.}
}
@misc{https://doi.org/10.48550/arxiv.2211.08545,
  author    = {Chang, Shuaichen and Palzer, David and Li, Jialin and Fosler-Lussier, Eric and Xiao, Ningchuan},
  year      = 2022,
  title     = {MapQA: A Dataset for Question Answering on Choropleth Maps},
  publisher = {arXiv},
  doi       = {10.48550/ARXIV.2211.08545},
  url       = {https://arxiv.org/abs/2211.08545},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@inproceedings{chen-etal-2021-geoqa,
  author    = {Chen, Jiaqi  and Tang, Jianheng  and Qin, Jinghui  and Liang, Xiaodan  and Liu, Lingbo  and Xing, Eric  and Lin, Liang},
  year      = 2021,
  month     = aug,
  title     = {{G}eo{QA}: A Geometric Question Answering Benchmark Towards Multimodal Numerical Reasoning},
  booktitle = {Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  publisher = {Association for Computational Linguistics},
  address   = {Online},
  pages     = {513--523},
  doi       = {10.18653/v1/2021.findings-acl.46},
  url       = {https://aclanthology.org/2021.findings-acl.46}
}
@book{chevallier2016strategic,
  author    = {Chevallier, Arnaud},
  year      = 2016,
  title     = {Strategic thinking in complex problem solving},
  publisher = {Oxford University Press}
}
@inproceedings{colas-etal-2020-tutorialvqa,
  author    = {Colas, Anthony  and Kim, Seokhwan  and Dernoncourt, Franck  and Gupte, Siddhesh  and Wang, Zhe  and Kim, Doo Soon},
  year      = 2020,
  month     = may,
  title     = {{T}utorial{VQA}: Question Answering Dataset for Tutorial Videos},
  booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},
  publisher = {European Language Resources Association},
  address   = {Marseille, France},
  pages     = {5450--5455},
  isbn      = {979-10-95546-34-4},
  url       = {https://aclanthology.org/2020.lrec-1.670},
  abstract  = {Despite the number of currently available datasets on video-question answering, there still remains a need for a dataset involving multi-step and non-factoid answers. Moreover, relying on video transcripts remains an under-explored topic. To adequately address this, we propose a new question answering task on instructional videos, because of their verbose and narrative nature. While previous studies on video question answering have focused on generating a short text as an answer, given a question and video clip, our task aims to identify a span of a video segment as an answer which contains instructional details with various granularities. This work focuses on screencast tutorial videos pertaining to an image editing program. We introduce a dataset, TutorialVQA, consisting of about 6,000 manually collected triples of (video, question, answer span). We also provide experimental results with several baseline algorithms using the video transcripts. The results indicate that the task is challenging and call for the investigation of new algorithms.},
  language  = {English}
}
@article{T5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  year    = 2020,
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  volume  = 21,
  number  = 140,
  pages   = {1--67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}
%% DocAI
@article{cui2021document,
  author  = {Cui, Lei and Xu, Yiheng and Lv, Tengchao and Wei, Furu},
  year    = 2021,
  title   = {Document ai: Benchmarks, models and applications},
  journal = {arXiv preprint arXiv:2111.08609}
}
@article{dasigi2019quoref,
  author  = {Dasigi, Pradeep and Liu, Nelson F and Marasovi{\'c}, Ana and Smith, Noah A and Gardner, Matt},
  year    = 2019,
  title   = {Quoref: A reading comprehension dataset with questions requiring coreferential reasoning},
  journal = {arXiv preprint arXiv:1908.05803}
}
@article{davis2022end,
  author  = {Davis, Brian and Morse, Bryan and Price, Bryan and Tensmeyer, Chris and Wigington, Curtis and Morariu, Vlad},
  year    = 2022,
  title   = {End-to-end Document Recognition and Understanding with Dessurt},
  journal = {arXiv e-prints},
  pages   = {arXiv--2203}
}
@article{dawid1982well,
  author    = {Dawid, A Philip},
  year      = 1982,
  title     = {The well-calibrated Bayesian},
  journal   = {Journal of the American Statistical Association},
  publisher = {Taylor \& Francis},
  volume    = 77,
  number    = 379,
  pages     = {605--610}
}
@article{degroot1983comparison,
  author    = {DeGroot, Morris H and Fienberg, Stephen E},
  year      = 1983,
  title     = {The comparison and evaluation of forecasters},
  journal   = {Journal of the Royal Statistical Society: Series D (The Statistician)},
  publisher = {Wiley Online Library},
  volume    = 32,
  number    = {1-2},
  pages     = {12--22}
}
%% Patterns & Transformers 


<<<<<<< HEAD
=======

>>>>>>> origin
% BERT
@inproceedings{devlin2018bert,
  author    = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year      = 2019,
  title     = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages     = {4171--4186}
}
@techreport{dimmick1992nist,
  author      = {Dimmick, DL and Garris, MD and Wilson, CL},
  year        = 1992,
  title       = {Nist special database 6. structured forms database 2},
  institution = {Technical report, National Institute od Standards and Technology. Advanced~…}
}
@inproceedings{dundar2007learning,
  author    = {Dundar, Murat and Krishnapuram, Balaji and Bi, Jinbo and Rao, R Bharat},
  year      = 2007,
  title     = {Learning classifiers when the training data is not IID.},
  booktitle = {IJCAI},
  volume    = 2007,
  pages     = {756--61}
}
@inproceedings{dutt-etal-2022-perkgqa,
  author    = {Dutt, Ritam  and Bhattacharjee, Kasturi  and Gangadharaiah, Rashmi  and Roth, Dan  and Rose, Carolyn},
  year      = 2022,
  month     = jul,
  title     = {{P}er{KGQA}: Question Answering over Personalized Knowledge Graphs},
  booktitle = {Findings of the Association for Computational Linguistics: NAACL 2022},
  publisher = {Association for Computational Linguistics},
  address   = {Seattle, United States},
  pages     = {253--268},
  doi       = {10.18653/v1/2022.findings-naacl.19},
  url       = {https://aclanthology.org/2022.findings-naacl.19},
  abstract  = {Previous studies on question answering over knowledge graphs have typically operated over a single knowledge graph (KG). This KG is assumed to be known a priori and is lever- aged similarly for all users{'} queries during inference. However, such an assumption is not applicable to real-world settings, such as health- care, where one needs to handle queries of new users over unseen KGs during inference. Furthermore, privacy concerns and high computational costs render it infeasible to query the single KG that has information about all users while answering a specific user{'}s query. The above concerns motivate our question answer- ing setting over personalized knowledge graphs (PERKGQA) where each user has restricted access to their KG. We observe that current state-of-the-art KGQA methods that require learning prior node representations fare poorly. We propose two complementary approaches, PATHCBR and PATHRGCN for PERKGQA. The former is a simple non-parametric technique that employs case-based reasoning, while the latter is a parametric approach using graph neural networks. Our proposed methods circumvent learning prior representations, can generalize to unseen KGs, and outperform strong baselines on an academic and an internal dataset by 6.5{\%} and 10.5{\%}.}
}
@article{SIMPSON_1949,
  author    = {E. H. SIMPSON},
  year      = 1949,
  month     = {apr},
  title     = {Measurement of Diversity},
  journal   = {Nature},
  publisher = {Springer Science and Business Media {LLC}},
  volume    = 163,
  number    = 4148,
  pages     = {688--688},
  doi       = {10.1038/163688a0},
  url       = {https://doi.org/10.1038\%2F163688a0}
}
@inproceedings{Zhu_2022,
  author    = {Fengbin Zhu and Wenqiang Lei and Fuli Feng and Chao Wang and Haozhou Zhang and Tat-Seng Chua},
  year      = 2022,
  month     = {oct},
  title     = {Towards Complex Document Understanding By Discrete Reasoning},
  booktitle = {Proceedings of the 30th {ACM} International Conference on Multimedia},
  publisher = {{ACM}},
  doi       = {10.1145/3503161.3548422},
  url       = {https://doi.org/10.1145}
}
@inbook{Flach2016,
  author    = {Flach, Peter A.},
  year      = 2016,
  title     = {Classifier Calibration},
  booktitle = {Encyclopedia of Machine Learning and Data Mining},
  publisher = {Springer US},
  address   = {Boston, MA},
  pages     = {1--8},
  doi       = {10.1007/978-1-4899-7502-7_900-1},
  isbn      = {978-1-4899-7502-7},
  url       = {https://doi.org/10.1007/978-1-4899-7502-7\%5F900-1},
  editor    = {Sammut, Claude and Webb, Geoffrey I.},
  abstract  = {Classifier calibration is concerned with the scale on which a classifier's scores are expressed. While a classifier ultimately maps instances to discrete classes, it is often beneficial to decompose this mapping into a scoring classifier which outputs one or more real-valued numbers and a decision rule which converts these numbers into predicted classes. For example, a linear classifiermight output a positive or negative score whose magnitude is proportional to the distance between the instance and the decision boundary, in which case the decision rule would be a simple threshold on that score. The advantage of calibrating these scores to a known, domain-independent scale is that the decision rule then also takes a domain-independent form and does not have to be learned. The best-known example of this occurs when the classifier's scores approximate, in a precise sense, the posterior probabilityover the classes; the main advantage of this is that the optimal decision rule is to predict the class that minimizes expected cost averaged over all possible true classes.The main methods to obtain calibrated scores are logistic calibration, which is a parametric method that assumes that the distances on either side of the decision boundary are normally distributed and a nonparametric alternative that is variously known as isotonic regression, the pool adjacent violators (PAV) method or the ROC convex hull (ROCCH) method.}
}
@inbook{Flach2016,
  author    = {Flach, Peter A.},
  year      = 2016,
  title     = {Classifier Calibration},
  booktitle = {Encyclopedia of Machine Learning and Data Mining},
  publisher = {Springer US},
  address   = {Boston, MA},
  pages     = {1--8},
  doi       = {10.1007/978-1-4899-7502-7_900-1},
  isbn      = {978-1-4899-7502-7},
  url       = {https://doi.org/10.1007/978-1-4899-7502-7\%5F900-1},
  editor    = {Sammut, Claude and Webb, Geoffrey I.},
  abstract  = {Classifier calibration is concerned with the scale on which a classifier's scores are expressed. While a classifier ultimately maps instances to discrete classes, it is often beneficial to decompose this mapping into a scoring classifier which outputs one or more real-valued numbers and a decision rule which converts these numbers into predicted classes. For example, a linear classifiermight output a positive or negative score whose magnitude is proportional to the distance between the instance and the decision boundary, in which case the decision rule would be a simple threshold on that score. The advantage of calibrating these scores to a known, domain-independent scale is that the decision rule then also takes a domain-independent form and does not have to be learned. The best-known example of this occurs when the classifier's scores approximate, in a precise sense, the posterior probabilityover the classes; the main advantage of this is that the optimal decision rule is to predict the class that minimizes expected cost averaged over all possible true classes.The main methods to obtain calibrated scores are logistic calibration, which is a parametric method that assumes that the distances on either side of the decision boundary are normally distributed and a nonparametric alternative that is variously known as isotonic regression, the pool adjacent violators (PAV) method or the ROC convex hull (ROCCH) method.}
}
@inproceedings{gallo2016deep,
  author       = {Gallo, Ignazio and Noce, Lucia and Zamberletti, Alessandro and Calefati, Alessandro},
  year         = 2016,
  title        = {Deep neural networks for page stream segmentation and classification},
  booktitle    = {2016 International Conference on Digital Image Computing: Techniques and Applications (DICTA)},
  pages        = {1--7},
  organization = {IEEE}
}
@article{garimella2016identification,
  author    = {Garimella, Siddharth},
  year      = 2016,
  title     = {Identification of receipts in a multi-receipt image using spectral clustering},
  journal   = {International Journal of Computer Applications},
  publisher = {Foundation of Computer Science},
  volume    = 155,
  number    = 2
}
@article{garncarek2020lambert,
  author  = {Garncarek, {\L}ukasz and Powalski, Rafa{\l} and Stanis{\l}awek, Tomasz and Topolski, Bartosz and Halama, Piotr and Grali{\'n}ski, Filip},
  year    = 2020,
  title   = {LAMBERT: Layout-Aware (Language) Modeling using BERT for information extraction},
  journal = {arXiv e-prints},
  pages   = {arXiv--2002}
}
@article{gebruDatasheetsDatasets2020,
  author        = {Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Daume{\'e} III, Hal and Crawford, Kate},
  year          = 2020,
  month         = jan,
  title         = {Datasheets for {{Datasets}}},
  journal       = {arXiv:1803.09010 [cs]},
  abstract      = {The machine learning community currently has no standardized process for documenting datasets. To address this gap, we propose datasheets for datasets. In the electronics industry, every component, no matter how simple or complex, is accompanied with a datasheet that describes its operating characteristics, test results, recommended uses, and other information. By analogy, we propose that every dataset be accompanied with a datasheet that documents its motivation, composition, collection process, recommended uses, and so on. Datasheets for datasets will facilitate better communication between dataset creators and dataset consumers, and encourage the machine learning community to prioritize transparency and accountability.},
  archiveprefix = {arXiv},
  eprint        = {1803.09010},
  eprinttype    = {arxiv},
  file          = {/Users/audrey/Zotero/storage/AUYFW6Q4/Gebru et al. - 2020 - Datasheets for Datasets.pdf},
  keywords      = {Computer Science - Artificial Intelligence,Computer Science - Databases,Computer Science - Machine Learning},
  language      = {en},
  primaryclass  = {cs}
}
@article{geifman2017selective,
  author  = {Geifman, Yonatan and El-Yaniv, Ran},
  year    = 2017,
  title   = {Selective classification for deep neural networks},
  journal = {Advances in neural information processing systems},
  volume  = 30
}
@inproceedings{glaser2021anonymization,
  author    = {Glaser, Ingo and Schamberger, Tom and Matthes, Florian},
  year      = 2021,
  title     = {Anonymization of german legal court rulings},
  booktitle = {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Law},
  pages     = {205--209}
}
@inproceedings{gordo2010bag,
  author       = {Gordo, Albert and Perronnin, Florent},
  year         = 2010,
  title        = {A bag-of-pages approach to unordered multi-page document classification},
  booktitle    = {2010 20th International Conference on Pattern Recognition},
  pages        = {1920--1923},
  organization = {IEEE}
}
@inproceedings{gordo2013document,
  author       = {Gordo, Albert and Rusinol, Mar{\c{c}}al and Karatzas, Dimosthenis and Bagdanov, Andrew D},
  year         = 2013,
  title        = {Document classification and page stream segmentation for digital mailroom applications},
  booktitle    = {2013 12th International Conference on Document Analysis and Recognition},
  pages        = {621--625},
  organization = {IEEE}
}
@article{gu2021unidoc,
  author  = {Gu, Jiuxiang and Kuen, Jason and Morariu, Vlad I and Zhao, Handong and Jain, Rajiv and Barmpalios, Nikolaos and Nenkova, Ani and Sun, Tong},
  year    = 2021,
  title   = {Unidoc: Unified pretraining framework for document understanding},
  journal = {Advances in Neural Information Processing Systems},
  volume  = 34,
  pages   = {39--50}
}
@inproceedings{gui-etal-2017-question,
  author    = {Gui, Lin  and Hu, Jiannan  and He, Yulan  and Xu, Ruifeng  and Lu, Qin  and Du, Jiachen},
  year      = 2017,
  month     = sep,
  title     = {A Question Answering Approach for Emotion Cause Extraction},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  publisher = {Association for Computational Linguistics},
  address   = {Copenhagen, Denmark},
  pages     = {1593--1602},
  doi       = {10.18653/v1/D17-1167},
  url       = {https://aclanthology.org/D17-1167},
  abstract  = {Emotion cause extraction aims to identify the reasons behind a certain emotion expressed in text. It is a much more difficult task compared to emotion classification. Inspired by recent advances in using deep memory networks for question answering (QA), we propose a new approach which considers emotion cause identification as a reading comprehension task in QA. Inspired by convolutional neural networks, we propose a new mechanism to store relevant context in different memory slots to model context information. Our proposed approach can extract both word level sequence features and lexical features. Performance evaluation shows that our method achieves the state-of-the-art performance on a recently released emotion cause dataset, outperforming a number of competitive baselines by at least 3.01{\%} in F-measure.}
}
%% Datasets
@inproceedings{jaume2019,
  author    = {Guillaume Jaume, Hazim Kemal Ekenel, Jean-Philippe Thiran},
  year      = 2019,
  title     = {FUNSD: A Dataset for Form Understanding in Noisy Scanned Documents},
  booktitle = {Accepted to ICDAR-OST}
}
@inproceedings{jaume2019,
  author    = {Guillaume Jaume, Hazim Kemal Ekenel, Jean-Philippe Thiran},
  year      = 2019,
  title     = {FUNSD: A Dataset for Form Understanding in Noisy Scanned Documents},
  booktitle = {Accepted to ICDAR-OST}
}
@inproceedings{guo2017calibration,
  author    = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
  year      = 2017,
  title     = {On Calibration of Modern Neural Networks},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
  location  = {Sydney, NSW, Australia},
  series    = {ICML'17},
  pages     = {1321–1330},
  abstract  = {Confidence calibration - the problem of predicting probability estimates representative of the true correctness likelihood - is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling - a single-parameter variant of Platt Scaling - is surprisingly effective at calibrating predictions.},
  numpages  = 10
}
@inproceedings{gupta-demner-fushman-2022-overview,
  author    = {Gupta, Deepak  and Demner-Fushman, Dina},
  year      = 2022,
  month     = may,
  title     = {Overview of the {M}ed{V}id{QA} 2022 Shared Task on Medical Video Question-Answering},
  booktitle = {Proceedings of the 21st Workshop on Biomedical Language Processing},
  publisher = {Association for Computational Linguistics},
  address   = {Dublin, Ireland},
  pages     = {264--274},
  doi       = {10.18653/v1/2022.bionlp-1.25},
  url       = {https://aclanthology.org/2022.bionlp-1.25},
  abstract  = {In this paper, we present an overview of the MedVidQA 2022 shared task, collocated with the 21st BioNLP workshop at ACL 2022. The shared task addressed two of the challenges faced by medical video question answering: (I) a video classification task that explores new approaches to medical video understanding (labeling), and (ii) a visual answer localization task. Visual answer localization refers to the identification of the relevant temporal segments (start and end timestamps) in the video where the answer to the medical question is being shown or illustrated. A total of thirteen teams participated in the shared task challenges, with eleven system descriptions submitted to the workshop. The descriptions present monomodal and multi-modal approaches developed for medical video classification and visual answer localization. This paper describes the tasks, the datasets, evaluation metrics, and baseline systems for both tasks. Finally, the paper summarizes the techniques and results of the evaluation of the various approaches explored by the participating teams.}
}
@misc{https://doi.org/10.48550/arxiv.1802.08218,
  author    = {Gurari, Danna and Li, Qing and Stangl, Abigale J. and Guo, Anhong and Lin, Chi and Grauman, Kristen and Luo, Jiebo and Bigham, Jeffrey P.},
  year      = 2018,
  title     = {VizWiz Grand Challenge: Answering Visual Questions from Blind People},
  publisher = {arXiv},
  doi       = {10.48550/ARXIV.1802.08218},
  url       = {https://arxiv.org/abs/1802.08218},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), Human-Computer Interaction (cs.HC), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@inproceedings{harley2015evaluation,
  author       = {Harley, Adam W and Ufkes, Alex and Derpanis, Konstantinos G},
  year         = 2015,
  title        = {Evaluation of deep convolutional nets for document image classification and retrieval},
  booktitle    = {2015 13th International Conference on Document Analysis and Recognition (ICDAR)},
  pages        = {991--995},
  organization = {IEEE}
}
@article{hendrickx2021machine,
  author  = {Hendrickx, Kilian and Perini, Lorenzo and Van der Plas, Dries and Meert, Wannes and Davis, Jesse},
  year    = 2021,
  title   = {Machine learning with a reject option: A survey},
  journal = {arXiv preprint arXiv:2107.11277}
}
@inproceedings{hopkins-etal-2019-semeval,
  author    = {Hopkins, Mark  and Le Bras, Ronan  and Petrescu-Prahova, Cristian  and Stanovsky, Gabriel  and Hajishirzi, Hannaneh  and Koncel-Kedziorski, Rik},
  year      = 2019,
  month     = jun,
  title     = {{S}em{E}val-2019 Task 10: Math Question Answering},
  booktitle = {Proceedings of the 13th International Workshop on Semantic Evaluation},
  publisher = {Association for Computational Linguistics},
  address   = {Minneapolis, Minnesota, USA},
  pages     = {893--899},
  doi       = {10.18653/v1/S19-2153},
  url       = {https://aclanthology.org/S19-2153},
  abstract  = {We report on the SemEval 2019 task on math question answering. We provided a question set derived from Math SAT practice exams, including 2778 training questions and 1082 test questions. For a significant subset of these questions, we also provided SMT-LIB logical form annotations and an interpreter that could solve these logical forms. Systems were evaluated based on the percentage of correctly answered questions. The top system correctly answered 45{\%} of the test questions, a considerable improvement over the 17{\%} random guessing baseline.}
}
@inproceedings{hu2019read+,
  author    = {Hu, Minghao and Wei, Furu and Peng, Yuxing and Huang, Zhen and Yang, Nan and Li, Dongsheng},
  year      = 2019,
  title     = {Read+ verify: Machine reading comprehension with unanswerable questions},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume    = 33,
  number    = {01},
  pages     = {6529--6537}
}
@inproceedings{hu-etal-2022-chef,
  author    = {Hu, Xuming  and Guo, Zhijiang  and Wu, GuanYu  and Liu, Aiwei  and Wen, Lijie  and Yu, Philip},
  year      = 2022,
  month     = jul,
  title     = {{CHEF}: A Pilot {C}hinese Dataset for Evidence-Based Fact-Checking},
  booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  publisher = {Association for Computational Linguistics},
  address   = {Seattle, United States},
  pages     = {3362--3376},
  doi       = {10.18653/v1/2022.naacl-main.246},
  url       = {https://aclanthology.org/2022.naacl-main.246},
  abstract  = {The explosion of misinformation spreading in the media ecosystem urges for automated fact-checking. While misinformation spans both geographic and linguistic boundaries, most work in the field has focused on English. Datasets and tools available in other languages, such as Chinese, are limited. In order to bridge this gap, we construct CHEF, the first CHinese Evidence-based Fact-checking dataset of 10K real-world claims. The dataset covers multiple domains, ranging from politics to public health, and provides annotated evidence retrieved from the Internet. Further, we develop established baselines and a novel approach that is able to model the evidence retrieval as a latent variable, allowing jointly training with the veracity prediction model in an end-to-end fashion. Extensive experiments show that CHEF will provide a challenging testbed for the development of fact-checking systems designed to retrieve and reason over non-English claims.}
}
@article{huang2022layoutlmv3,
  author  = {Huang, Yupan and Lv, Tengchao and Cui, Lei and Lu, Yutong and Wei, Furu},
  year    = 2022,
  title   = {LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking},
  journal = {arXiv preprint arXiv:2204.08387}
}
@inproceedings{huang2019icdar2019,
  author       = {Huang, Zheng and Chen, Kai and He, Jianhua and Bai, Xiang and Karatzas, Dimosthenis and Lu, Shijian and Jawahar, CV},
  year         = 2019,
  title        = {Icdar2019 competition on scanned receipt ocr and information extraction},
  booktitle    = {2019 International Conference on Document Analysis and Recognition (ICDAR)},
  pages        = {1516--1520},
  organization = {IEEE}
}
@inproceedings{jaume2019funsd,
  author       = {Jaume, Guillaume and Ekenel, Hazim Kemal and Thiran, Jean-Philippe},
  year         = 2019,
  title        = {Funsd: A dataset for form understanding in noisy scanned documents},
  booktitle    = {2019 International Conference on Document Analysis and Recognition Workshops (ICDARW)},
  volume       = 2,
  pages        = {1--6},
  organization = {IEEE}
}
@inproceedings{jin-etal-2019-pubmedqa,
  author    = {Jin, Qiao  and Dhingra, Bhuwan  and Liu, Zhengping  and Cohen, William  and Lu, Xinghua},
  year      = 2019,
  month     = nov,
  title     = {{P}ub{M}ed{QA}: A Dataset for Biomedical Research Question Answering},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  publisher = {Association for Computational Linguistics},
  address   = {Hong Kong, China},
  pages     = {2567--2577},
  doi       = {10.18653/v1/D19-1259},
  url       = {https://aclanthology.org/D19-1259},
  abstract  = {We introduce PubMedQA, a novel biomedical question answering (QA) dataset collected from PubMed abstracts. The task of PubMedQA is to answer research questions with yes/no/maybe (e.g.: Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?) using the corresponding abstracts. PubMedQA has 1k expert-annotated, 61.2k unlabeled and 211.3k artificially generated QA instances. Each PubMedQA instance is composed of (1) a question which is either an existing research article title or derived from one, (2) a context which is the corresponding abstract without its conclusion, (3) a long answer, which is the conclusion of the abstract and, presumably, answers the research question, and (4) a yes/no/maybe answer which summarizes the conclusion. PubMedQA is the first QA dataset where reasoning over biomedical research texts, especially their quantitative contents, is required to answer the questions. Our best performing model, multi-phase fine-tuning of BioBERT with long answer bag-of-word statistics as additional supervision, achieves 68.1{\%} accuracy, compared to single human performance of 78.0{\%} accuracy and majority-baseline of 55.2{\%} accuracy, leaving much room for improvement. PubMedQA is publicly available at https://pubmedqa.github.io.}
}
@inproceedings{wenger2020calibration,
  author    = {Jonathan Wenger and Hedvig Kjellstr{\"{o}}m and Rudolph Triebel},
  year      = 2020,
  title     = {Non-Parametric Calibration for Classification},
  booktitle = {23rd International Conference on Artificial Intelligence and Statistics ({AISTATS})},
  publisher = {{PMLR}},
  pages     = {178--190},
  keywords  = {calibration, non-parametric, gaussian processes, classification},
  pdf       = {http://proceedings.mlr.press/v108/wenger20a/wenger20a.pdf}
}
@inproceedings{wenger2020calibration,
  author    = {Jonathan Wenger and Hedvig Kjellstr{\"{o}}m and Rudolph Triebel},
  year      = 2020,
  title     = {Non-Parametric Calibration for Classification},
  booktitle = {23rd International Conference on Artificial Intelligence and Statistics ({AISTATS})},
  publisher = {{PMLR}},
  pages     = {178--190},
  keywords  = {calibration, non-parametric, gaussian processes, classification},
  pdf       = {http://proceedings.mlr.press/v108/wenger20a/wenger20a.pdf}
}
@inproceedings{kacupaj-etal-2021-conversational,
  author    = {Kacupaj, Endri  and Plepi, Joan  and Singh, Kuldeep  and Thakkar, Harsh  and Lehmann, Jens  and Maleshkova, Maria},
  year      = 2021,
  month     = apr,
  title     = {Conversational Question Answering over Knowledge Graphs with Transformer and Graph Attention Networks},
  booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  publisher = {Association for Computational Linguistics},
  address   = {Online},
  pages     = {850--862},
  doi       = {10.18653/v1/2021.eacl-main.72},
  url       = {https://aclanthology.org/2021.eacl-main.72},
  abstract  = {This paper addresses the task of (complex) conversational question answering over a knowledge graph. For this task, we propose LASAGNE (muLti-task semAntic parSing with trAnsformer and Graph atteNtion nEworks). It is the first approach, which employs a transformer architecture extended with Graph Attention Networks for multi-task neural semantic parsing. LASAGNE uses a transformer model for generating the base logical forms, while the Graph Attention model is used to exploit correlations between (entity) types and predicates to produce node representations. LASAGNE also includes a novel entity recognition module which detects, links, and ranks all relevant entities in the question context. We evaluate LASAGNE on a standard dataset for complex sequential question answering, on which it outperforms existing baselines averaged on all question types. Specifically, we show that LASAGNE improves the F1-score on eight out of ten question types; in some cases, the increase is more than 20{\%} compared to state of the art (SotA).}
}
@inproceedings{kamath2020selective,
  author    = {Kamath, Amita and Jia, Robin and Liang, Percy},
  year      = 2020,
  title     = {Selective Question Answering under Domain Shift},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages     = {5684--5696}
}
@inproceedings{kamath:hal-01759306,
  author      = {Kamath, Sanjay and Grau, Brigitte and Ma, Yue},
  year        = 2018,
  month       = Apr,
  title       = {{Verification of the Expected Answer Type for Biomedical Question Answering}},
  booktitle   = {{First International Workshop on Hybrid Question Answering with Structured and Unstructured Knowledge (HQA'18)}},
  publisher   = {{ACM Press}},
  address     = {Lyon, France},
  series      = {WWW '18 Companion Proceedings of the The Web Conference 2018},
  pages       = {1093--1097},
  doi         = {10.1145/3184558.3191542},
  url         = {https://hal.science/hal-01759306},
  pdf         = {https://hal.science/hal-01759306/file/p1093-kamath.pdf},
  hal_id      = {hal-01759306},
  hal_version = {v1}
}
@article{kanchi2022emmdocclassifier,
  author    = {Kanchi, Shrinidhi and Pagani, Alain and Mokayed, Hamam and Liwicki, Marcus and Stricker, Didier and Afzal, Muhammad Zeshan},
  year      = 2022,
  title     = {EmmDocClassifier: Efficient Multimodal Document Image Classifier for Scarce Data},
  journal   = {Applied Sciences},
  publisher = {MDPI},
  volume    = 12,
  number    = 3,
  pages     = 1457
}
@article{katti2018chargrid,
  author  = {Katti, Anoop Raveendra and Reisswig, Christian and Guder, Cordula and Brarda, Sebastian and Bickel, Steffen and H{\"o}hne, Johannes and Faddoul, Jean Baptiste},
  year    = 2018,
  title   = {Chargrid: Towards understanding 2d documents},
  journal = {arXiv preprint arXiv:1809.08799}
}
@article{katti2018chargrid,
  author  = {Katti, Anoop Raveendra and Reisswig, Christian and Guder, Cordula and Brarda, Sebastian and Bickel, Steffen and H{\"o}hne, Johannes and Faddoul, Jean Baptiste},
  year    = 2018,
  title   = {Chargrid: Towards understanding 2d documents},
  journal = {arXiv preprint arXiv:1809.08799}
}
@article{kim2021donut,
  author  = {Kim, Geewook and Hong, Teakgyu and Yim, Moonbin and Park, Jinyoung and Yim, Jinyeong and Hwang, Wonseok and Yun, Sangdoo and Han, Dongyoon and Park, Seunghyun},
  year    = 2021,
  title   = {Donut: Document Understanding Transformer without OCR},
  journal = {arXiv preprint arXiv:2111.15664}
}
@misc{projectid,
  author    = {Kirsch, Andreas},
  year      = 2023,
  title     = {Player of Jeopardy: ChatGPT Evaluation},
  booktitle = {GitHub},
  url       = {https://github.com/BlackHC/player\%5Fof\%5Fjeopardy}
}
@inproceedings{kull2019beyond,
  author    = {Kull, Meelis and Nieto, Miquel Perello and K{\"a}ngsepp, Markus and Silva Filho, Telmo and Song, Hao and Flach, Peter},
  year      = 2019,
  title     = {Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with {Dirichlet} calibration},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {12316--12326}
}
@inproceedings{kull2019beyond,
  author    = {Kull, Meelis and Nieto, Miquel Perello and K{\"a}ngsepp, Markus and Silva Filho, Telmo and Song, Hao and Flach, Peter},
  year      = 2019,
  title     = {Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with dirichlet calibration},
  booktitle = {Advances in neural information processing systems},
  pages     = {12316--12326}
}
@inproceedings{kull2019beyond,
  author    = {Kull, Meelis and Nieto, Miquel Perello and K{\"a}ngsepp, Markus and Silva Filho, Telmo and Song, Hao and Flach, Peter},
  year      = 2019,
  title     = {Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with {Dirichlet} calibration},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {12316--12326}
}
@inproceedings{kull2019beyond,
  author    = {Kull, Meelis and Nieto, Miquel Perello and K{\"a}ngsepp, Markus and Silva Filho, Telmo and Song, Hao and Flach, Peter},
  year      = 2019,
  title     = {Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with dirichlet calibration},
  booktitle = {Advances in neural information processing systems},
  pages     = {12316--12326}
}
@inproceedings{kumar2018trainable,
  author       = {Kumar, Aviral and Sarawagi, Sunita and Jain, Ujjwal},
  year         = 2018,
  title        = {Trainable calibration measures for neural networks from kernel mean embeddings},
  booktitle    = {International Conference on Machine Learning},
  pages        = {2805--2814},
  organization = {PMLR}
}
@inproceedings{kumar2013unsupervised,
  author       = {Kumar, Jayant and Doermann, David},
  year         = 2013,
  title        = {Unsupervised classification of structurally similar document images},
  booktitle    = {2013 12th International Conference on Document Analysis and Recognition},
  pages        = {1225--1229},
  organization = {IEEE}
}
@article{kumar2014structural,
  author    = {Kumar, Jayant and Ye, Peng and Doermann, David},
  year      = 2014,
  title     = {Structural similarity for document image classification and retrieval},
  journal   = {Pattern Recognition Letters},
  publisher = {Elsevier},
  volume    = 43,
  pages     = {119--126}
}
@article{kwiatkowski2019natural,
  author  = {Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  year    = 2019,
  title   = {Natural questions: a benchmark for question answering research},
  journal = {Transactions of the Association for Computational Linguistics}
}
@inproceedings{lakomkin-etal-2018-kt,
  author    = {Lakomkin, Egor  and Magg, Sven  and Weber, Cornelius  and Wermter, Stefan},
  year      = 2018,
  month     = nov,
  title     = {{KT}-Speech-Crawler: Automatic Dataset Construction for Speech Recognition from {Y}ou{T}ube Videos},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  publisher = {Association for Computational Linguistics},
  address   = {Brussels, Belgium},
  pages     = {90--95},
  doi       = {10.18653/v1/D18-2016},
  url       = {https://aclanthology.org/D18-2016},
  abstract  = {We describe KT-Speech-Crawler: an approach for automatic dataset construction for speech recognition by crawling YouTube videos. We outline several filtering and post-processing steps, which extract samples that can be used for training end-to-end neural speech recognition systems. In our experiments, we demonstrate that a single-core version of the crawler can obtain around 150 hours of transcribed speech within a day, containing an estimated 3.5{\%} word error rate in the transcriptions. Automatically collected samples contain reading and spontaneous speech recorded in various conditions including background noise and music, distant microphone recordings, and a variety of accents and reverberation. When training a deep neural network on speech recognition, we observed around 40{\%} word error rate reduction on the Wall Street Journal dataset by integrating 200 hours of the collected samples into the training set.}
}
@inproceedings{larson-etal-2023-evaluation,
  author    = {Larson, Stefan  and Lim, Gordon  and Leach, Kevin},
  year      = 2023,
  month     = may,
  title     = {On Evaluation of Document Classification with {RVL}-{CDIP}},
  booktitle = {Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},
  publisher = {Association for Computational Linguistics},
  address   = {Dubrovnik, Croatia},
  pages     = {2665--2678},
  url       = {https://aclanthology.org/2023.eacl-main.195},
  abstract  = {The RVL-CDIP benchmark is widely used for measuring performance on the task of document classification. Despite its widespread use, we reveal several undesirable characteristics of the RVL-CDIP benchmark. These include (1) substantial amounts of label noise, which we estimate to be 8.1{\%} (ranging between 1.6{\%} to 16.9{\%} per document category); (2) presence of many ambiguous or multi-label documents; (3) a large overlap between test and train splits, which can inflate model performance metrics; and (4) presence of sensitive personally-identifiable information like US Social Security numbers (SSNs). We argue that there is a risk in using RVL-CDIP for benchmarking document classifiers, as its limited scope, presence of errors (state-of-the-art models now achieve accuracy error rates that are within our estimated label error rate), and lack of diversity make it less than ideal for benchmarking. We further advocate for the creation of a new document classification benchmark, and provide recommendations for what characteristics such a resource should include.}
}
@inproceedings{larson2022evaluating,
  author    = {Larson, Stefan and Lim, Gordon and Ai, Yutong and Kuang, David and Leach, Kevin},
  year      = 2022,
  title     = {Evaluating Out-of-Distribution Performance on Document Image Classifiers},
  booktitle = {Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track}
}
@inproceedings{larson2023labelnoise,
  author    = {Larson, Stefan and Lim, Gordon and Leach, Kevin},
  year      = 2023,
  title     = {On Evaluation of Document Classification with RVL-CDIP},
  booktitle = {Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics (EACL)}
}
@inproceedings{lee2022pix2struct,
  author       = {Lee, Kenton and Joshi, Mandar and Turc, Iulia Raluca and Hu, Hexiang and Liu, Fangyu and Eisenschlos, Julian Martin and Khandelwal, Urvashi and Shaw, Peter and Chang, Ming-Wei and Toutanova, Kristina},
  year         = 2023,
  title        = {Pix2struct: Screenshot parsing as pretraining for visual language understanding},
  booktitle    = {International Conference on Machine Learning},
  pages        = {18893--18912},
  organization = {PMLR}
}
@inproceedings{lei-etal-2018-tvqa,
  author    = {Lei, Jie  and Yu, Licheng  and Bansal, Mohit  and Berg, Tamara},
  year      = 2018,
  month     = oct # {-} # nov,
  title     = {{TVQA}: Localized, Compositional Video Question Answering},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  publisher = {Association for Computational Linguistics},
  address   = {Brussels, Belgium},
  pages     = {1369--1379},
  doi       = {10.18653/v1/D18-1167},
  url       = {https://aclanthology.org/D18-1167},
  abstract  = {Recent years have witnessed an increasing interest in image-based question-answering (QA) tasks. However, due to data limitations, there has been much less work on video-based QA. In this paper, we present TVQA, a large-scale video QA dataset based on 6 popular TV shows. TVQA consists of 152,545 QA pairs from 21,793 clips, spanning over 460 hours of video. Questions are designed to be compositional in nature, requiring systems to jointly localize relevant moments within a clip, comprehend subtitle-based dialogue, and recognize relevant visual concepts. We provide analyses of this new dataset as well as several baselines and a multi-stream end-to-end trainable neural network framework for the TVQA task. The dataset is publicly available at \url{http://tvqa.cs.unc.edu}.}
}
@inproceedings{levenshtein1966binary,
  author       = {Levenshtein, Vladimir I and others},
  year         = 1966,
  title        = {Binary codes capable of correcting deletions, insertions, and reversals},
  booktitle    = {Soviet physics doklady},
  volume       = 10,
  number       = 8,
  pages        = {707--710},
  organization = {Soviet Union}
}
@inproceedings{lewis2006building,
  author    = {Lewis, David and Agam, Gady and Argamon, Shlomo and Frieder, Ophir and Grossman, David and Heard, Jefferson},
  year      = 2006,
  title     = {Building a test collection for complex document information processing},
  booktitle = {Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval},
  pages     = {665--666}
}
@inproceedings{li2022multispanqa,
  author    = {Li, Haonan and Tomko, Martin and Vasardani, Maria and Baldwin, Timothy},
  year      = 2022,
  title     = {MultiSpanQA: A Dataset for Multi-Span Question Answering},
  booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages     = {1250--1260}
}
@inproceedings{li-etal-2021-mlec,
  author    = {Li, Jing  and Zhong, Shangping  and Chen, Kaizhi},
  year      = 2021,
  month     = nov,
  title     = {{MLEC-QA}: {A} {C}hinese {M}ulti-{C}hoice {B}iomedical {Q}uestion {A}nswering {D}ataset},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  publisher = {Association for Computational Linguistics},
  address   = {Online and Punta Cana, Dominican Republic},
  pages     = {8862--8874},
  doi       = {10.18653/v1/2021.emnlp-main.698},
  url       = {https://aclanthology.org/2021.emnlp-main.698},
  abstract  = {Question Answering (QA) has been successfully applied in scenarios of human-computer interaction such as chatbots and search engines. However, for the specific biomedical domain, QA systems are still immature due to expert-annotated datasets being limited by category and scale. In this paper, we present MLEC-QA, the largest-scale Chinese multi-choice biomedical QA dataset, collected from the National Medical Licensing Examination in China. The dataset is composed of five subsets with 136,236 biomedical multi-choice questions with extra materials (images or tables) annotated by human experts, and first covers the following biomedical sub-fields: Clinic, Stomatology, Public Health, Traditional Chinese Medicine, and Traditional Chinese Medicine Combined with Western Medicine. We implement eight representative control methods and open-domain QA methods as baselines. Experimental results demonstrate that even the current best model can only achieve accuracies between 40{\%} to 55{\%} on five subsets, especially performing poorly on questions that require sophisticated reasoning ability. We hope the release of the MLEC-QA dataset can serve as a valuable resource for research and evaluation in open-domain QA, and also make advances for biomedical QA systems.}
}
@inproceedings{li2022dit,
  author    = {Li, Junlong and Xu, Yiheng and Lv, Tengchao and Cui, Lei and Zhang, Cha and Wei, Furu},
  year      = 2022,
  title     = {Dit: Self-supervised pre-training for document image transformer},
  booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
  pages     = {3530--3539}
}
@inproceedings{li2021selfdoc,
  author    = {Li, Peizhao and Gu, Jiuxiang and Kuen, Jason and Morariu, Vlad I and Zhao, Handong and Jain, Rajiv and Manjunatha, Varun and Liu, Hongfu},
  year      = 2021,
  title     = {Selfdoc: Self-supervised document representation learning},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {5652--5660}
}
@article{lin2022teaching,
  author  = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  year    = 2022,
  title   = {Teaching Models to Express Their Uncertainty in Words},
  journal = {arXiv preprint arXiv:2205.14334}
}
@inproceedings{liu-wan-2021-codeqa-question,
  author    = {Liu, Chenxiao  and Wan, Xiaojun},
  year      = 2021,
  month     = nov,
  title     = {{C}ode{QA}: A Question Answering Dataset for Source Code Comprehension},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2021},
  publisher = {Association for Computational Linguistics},
  address   = {Punta Cana, Dominican Republic},
  pages     = {2618--2632},
  doi       = {10.18653/v1/2021.findings-emnlp.223},
  url       = {https://aclanthology.org/2021.findings-emnlp.223},
  abstract  = {We propose CodeQA, a free-form question answering dataset for the purpose of source code comprehension: given a code snippet and a question, a textual answer is required to be generated. CodeQA contains a Java dataset with 119,778 question-answer pairs and a Python dataset with 70,085 question-answer pairs. To obtain natural and faithful questions and answers, we implement syntactic rules and semantic analysis to transform code comments into question-answer pairs. We present the construction process and conduct systematic analysis of our dataset. Experiment results achieved by several neural baselines on our dataset are shown and discussed. While research on question-answering and machine reading comprehension develops rapidly, few prior work has drawn attention to code question answering. This new dataset can serve as a useful research benchmark for source code comprehension.}
}
@inproceedings{liu-etal-2019-xqa,
  author    = {Liu, Jiahua  and Lin, Yankai  and Liu, Zhiyuan  and Sun, Maosong},
  year      = 2019,
  month     = jul,
  title     = {{XQA}: A Cross-lingual Open-domain Question Answering Dataset},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  publisher = {Association for Computational Linguistics},
  address   = {Florence, Italy},
  pages     = {2358--2368},
  doi       = {10.18653/v1/P19-1227},
  url       = {https://aclanthology.org/P19-1227},
  abstract  = {Open-domain question answering (OpenQA) aims to answer questions through text retrieval and reading comprehension. Recently, lots of neural network-based models have been proposed and achieved promising results in OpenQA. However, the success of these models relies on a massive volume of training data (usually in English), which is not available in many other languages, especially for those low-resource languages. Therefore, it is essential to investigate cross-lingual OpenQA. In this paper, we construct a novel dataset XQA for cross-lingual OpenQA research. It consists of a training set in English as well as development and test sets in eight other languages. Besides, we provide several baseline systems for cross-lingual OpenQA, including two machine translation-based methods and one zero-shot cross-lingual method (multilingual BERT). Experimental results show that the multilingual BERT model achieves the best results in almost all target languages, while the performance of cross-lingual OpenQA is still much lower than that of English. Our analysis indicates that the performance of cross-lingual OpenQA is related to not only how similar the target language and English are, but also how difficult the question set of the target language is. The XQA dataset is publicly available at http://github.com/thunlp/XQA.}
}
@inproceedings{ijcai2020p0501,
  author    = {Liu, Jian and Cui, Leyang and Liu, Hanmeng and Huang, Dandan and Wang, Yile and Zhang, Yue},
  year      = 2020,
  month     = 7,
  title     = {LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning},
  booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, {IJCAI-20}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  pages     = {3622--3628},
  doi       = {10.24963/ijcai.2020/501},
  url       = {https://doi.org/10.24963/ijcai.2020/501},
  note      = {Main track},
  editor    = {Christian Bessiere}
}
@inproceedings{livathinos2021robust,
  author    = {Livathinos, Nikolaos and Berrospi, Cesar and Lysak, Maksym and Kuropiatnyk, Viktor and Nassar, Ahmed and Carvalho, Andre and Dolfi, Michele and Auer, Christoph and Dinkla, Kasper and Staar, Peter},
  year      = 2021,
  title     = {Robust PDF document conversion using recurrent neural networks},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume    = 35,
  number    = 17,
  pages     = {15137--15145}
}
@article{long2005image,
  author    = {Long, L Rodney and Antani, Sameer K and Thoma, George R},
  year      = 2005,
  title     = {Image informatics at a national research center},
  journal   = {Computerized Medical Imaging and Graphics},
  publisher = {Elsevier},
  volume    = 29,
  number    = {2-3},
  pages     = {171--193}
}
@misc{https://doi.org/10.48550/arxiv.2007.15207,
  author    = {Longpre, Shayne and Lu, Yi and Daiber, Joachim},
  year      = 2020,
  title     = {MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering},
  publisher = {arXiv},
  doi       = {10.48550/ARXIV.2007.15207},
  url       = {https://arxiv.org/abs/2007.15207},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords  = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@inproceedings{Garncarek2021LAMBERTLL,
  author    = {Lukasz Garncarek and Rafal Powalski and Tomasz Stanislawek and Bartosz Topolski and Piotr Halama and Michał Turski and Filip Graliński},
  year      = 2021,
  title     = {LAMBERT: Layout-Aware language Modeling using BERT for information extraction},
  booktitle = {ICDAR}
}
@article{luz2023sequence,
  author    = {Luz de Araujo, Pedro H and de Almeida, Ana Paula GS and Ataides Braz, Fabricio and Correia da Silva, Nilton and de Barros Vidal, Flavio and de Campos, Teofilo E},
  year      = 2023,
  title     = {Sequence-aware multimodal page classification of Brazilian legal documents},
  journal   = {International Journal on Document Analysis and Recognition (IJDAR)},
  publisher = {Springer},
  volume    = 26,
  number    = 1,
  pages     = {33--49}
}
@inproceedings{moller-etal-2020-covid,
  author    = {M{\"o}ller, Timo  and Reina, Anthony  and Jayakumar, Raghavan  and Pietsch, Malte},
  year      = 2020,
  month     = jul,
  title     = {{COVID-QA}: A Question Answering Dataset for {COVID}-19},
  booktitle = {Proceedings of the 1st Workshop on {NLP} for {COVID-19} at {ACL} 2020},
  publisher = {Association for Computational Linguistics},
  address   = {Online},
  url       = {https://aclanthology.org/2020.nlpcovid19-acl.18},
  abstract  = {We present COVID-QA, a Question Answering dataset consisting of 2,019 question/answer pairs annotated by volunteer biomedical experts on scientific articles related to COVID-19. To evaluate the dataset we compared a RoBERTa base model fine-tuned on SQuAD with the same model trained on SQuAD and our COVID-QA dataset. We found that the additional training on this domain-specific data leads to significant gains in performance. Both the trained model and the annotated dataset have been open-sourced at: https://github.com/deepset-ai/COVID-QA}
}
@inproceedings{10.1007/978-3-031-06555-2_44,
  author    = {Mahamoud, Ibrahim Souleiman and Coustaty, Micka{\"e}l and Joseph, Aur{\'e}lie and d'Andecy, Vincent Poulain and Ogier, Jean-Marc},
  year      = 2022,
  title     = {QAlayout: Question Answering Layout Based on Multimodal Attention for Visual Question Answering on Corporate Document},
  booktitle = {Document Analysis Systems},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {659--673},
  isbn      = {978-3-031-06555-2},
  editor    = {Uchida, Seiichi and Barney, Elisa and Eglin, V{\'e}ronique},
  abstract  = {The extraction of information from corporate documents is increasing in the research field both for its economic aspect and a scientific challenge. To extract this information the use of textual and visual content becomes unavoidable to understand the inherent information of the image. The information to be extracted is most often fixed beforehand (i.e. classification of words by date, total amount, etc.). The information to be extracted is evolving, so we would not like to be restricted to predefine word classes. We would like to question a document such as ``which is the address of invoicing?'' as we can have several addresses in an invoice. We formulate our request as a question and our model will try to answer. Our model got the result 77.65{\%} on the Docvqa dataset while drastically reducing the number of model parameters to allow us to use it in an industrial context and we use an attention model using several modalities that help us in the interpertation of the results obtained. Our other contribution in this paper is a new dataset for Visual Question answering on corporate document of invoices from RVL-CDIP [8]. The public data on corporate documents are less present in the state-of-the-art, this contribution allow us to test our models to the invoice data with the VQA methods.}
}
@article{maini2022augraphy,
  author  = {Maini, Samay and Groleau, Alexander and Chee, Kok Wei and Larson, Stefan and Boarman, Jonathan},
  year    = 2022,
  title   = {Augraphy: A data augmentation library for document images},
  journal = {arXiv preprint arXiv:2208.14558}
}
@inproceedings{mathew2022infographicvqa,
  author    = {Mathew, Minesh and Bagal, Viraj and Tito, Rub{\`e}n and Karatzas, Dimosthenis and Valveny, Ernest and Jawahar, CV},
  year      = 2022,
  title     = {InfographicVQA},
  booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages     = {1697--1706}
}
@article{mathew2020document,
  author  = {Mathew, Minesh and Tito, Ruben and Karatzas, Dimosthenis and Manmatha, R and Jawahar, CV},
  year    = 2020,
  title   = {Document visual question answering challenge 2020},
  journal = {arXiv preprint arXiv:2008.08899}
}
@misc{https://doi.org/10.48550/arxiv.2004.10645,
  author    = {Min, Sewon and Michael, Julian and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  year      = 2020,
  title     = {AmbigQA: Answering Ambiguous Open-domain Questions},
  publisher = {arXiv},
  doi       = {10.48550/ARXIV.2004.10645},
  url       = {https://arxiv.org/abs/2004.10645},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords  = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@misc{li2020docbank,
  author        = {Minghao Li and Yiheng Xu and Lei Cui and Shaohan Huang and Furu Wei and Zhoujun Li and Ming Zhou},
  year          = 2020,
  title         = {DocBank: A Benchmark Dataset for Document Layout Analysis},
  eprint        = {2006.01038},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@inproceedings{mishra2019ocr,
  author       = {Mishra, Anand and Shekhar, Shashank and Singh, Ajeet Kumar and Chakraborty, Anirban},
  year         = 2019,
  title        = {Ocr-vqa: Visual question answering by reading text in images},
  booktitle    = {2019 international conference on document analysis and recognition (ICDAR)},
  pages        = {947--952},
  organization = {IEEE}
}
@inproceedings{mishra-etal-2022-numglue,
  author    = {Mishra, Swaroop  and Mitra, Arindam  and Varshney, Neeraj  and Sachdeva, Bhavdeep  and Clark, Peter  and Baral, Chitta  and Kalyan, Ashwin},
  year      = 2022,
  month     = may,
  title     = {{N}um{GLUE}: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  publisher = {Association for Computational Linguistics},
  address   = {Dublin, Ireland},
  pages     = {3505--3523},
  doi       = {10.18653/v1/2022.acl-long.246},
  url       = {https://aclanthology.org/2022.acl-long.246},
  abstract  = {Given the ubiquitous nature of numbers in text, reasoning with numbers to perform simple calculations is an important skill of AI systems. While many datasets and models have been developed to this end, state-of-the-art AI systems are brittle; failing to perform the underlying mathematical reasoning when they appear in a slightly different scenario. Drawing inspiration from GLUE that was proposed in the context of natural language understanding, we propose NumGLUE, a multi-task benchmark that evaluates the performance of AI systems on eight different tasks, that at their core require simple arithmetic understanding. We show that this benchmark is far from being solved with neural models including state-of-the-art large-scale language models performing significantly worse than humans (lower by 46.4 {\%}). Further, NumGLUE promotes sharing knowledge across tasks, especially those with limited training data as evidenced by the superior performance (average gain of 3.4 {\%} on each task) when a model is jointly trained on all the tasks as opposed to task-specific modeling. Finally, we hope that NumGLUE will encourage systems that perform robust and general arithmetic reasoning within language, a first step towards being able to perform more complex mathematical reasoning.}
}
@inproceedings{mungmeeprued2022tab,
  author    = {Mungmeeprued, Thisanaporn and Ma, Yuxin and Mehta, Nisarg and Lipani, Aldo},
  year      = 2022,
  title     = {Tab this folder of documents: page stream segmentation of business documents},
  booktitle = {Proceedings of the 22nd ACM Symposium on Document Engineering},
  pages     = {1--10}
}
@inproceedings{munirtowards,
  author    = {Munir, Muhammad Akhtar and Khan, Muhammad Haris and Sarfraz, M Saquib and Ali, Mohsen},
  year      = 2022,
  title     = {Towards Improving Calibration in Object Detection Under Domain Shift},
  booktitle = {Advances in Neural Information Processing Systems}
}
@inproceedings{naeini2015obtaining,
  author    = {Naeini, Mahdi Pakdaman and Cooper, Gregory and Hauskrecht, Milos},
  year      = 2015,
  title     = {Obtaining well calibrated probabilities using {Bayesian} binning},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume    = 29,
  number    = 1
}
@inproceedings{nassar2022tableformer,
  author    = {Nassar, Ahmed and Livathinos, Nikolaos and Lysak, Maksym and Staar, Peter},
  year      = 2022,
  title     = {Tableformer: Table structure understanding with transformers},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {4614--4623}
}
@inproceedings{nguyen2021skim,
  author    = {Nguyen, Laura and Scialom, Thomas and Staiano, Jacopo and Piwowarski, Benjamin},
  year      = 2021,
  title     = {Skim-Attention: Learning to Focus via Document Layout},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2021},
  pages     = {2413--2427}
}
@article{nguyen2016ms,
  author  = {Nguyen, Tri and Rosenberg, Mir and Song, Xia and Gao, Jianfeng and Tiwary, Saurabh and Majumder, Rangan and Deng, Li},
  year    = 2016,
  title   = {MS MARCO: A human generated machine reading comprehension dataset},
  journal = {choice},
  volume  = 2640,
  pages   = 660
}
@inbook{ToE,
  author      = {Nicholas Rescher},
  year        = 2013,
  title       = {Chapter 1: Holistic explanation and the idea of a grand unified theory},
  booktitle   = {Volume 9 Studies in Metaphilosophy},
  publisher   = {De Gruyter},
  address     = {Berlin, Boston},
  pages       = {1--10},
  isbn        = 9783110326420,
  url         = {https://doi.org/10.1515/9783110326420.1},
  lastchecked = {2022-10-28}
}
@inproceedings{niculescu2005predicting,
  author    = {Niculescu-Mizil, Alexandru and Caruana, Rich},
  year      = 2005,
  title     = {Predicting good probabilities with supervised learning},
  booktitle = {Proceedings of the 22nd International Conference on Machine learning},
  pages     = {625--632}
}
@inproceedings{nikas2020two,
  author    = {Nikas, Christos and Fafalios, Pavlos and Tzitzikas, Yannis},
  year      = 2020,
  title     = {Two-stage Semantic Answer Type Prediction for Question Answering using BERT and Class-Specificity Rewarding.},
  booktitle = {SMARTat ISWC},
  pages     = {19--28}
}
@inproceedings{nixon2019measuring,
  author    = {Nixon, Jeremy and Dusenberry, Michael W and Zhang, Linchuan and Jerfel, Ghassen and Tran, Dustin},
  year      = 2019,
  title     = {Measuring Calibration in Deep Learning.},
  booktitle = {CVPR Workshops},
  volume    = 2,
  number    = 7
}
@inproceedings{nixon2019measuring,
  author    = {Nixon, Jeremy and Dusenberry, Michael W and Zhang, Linchuan and Jerfel, Ghassen and Tran, Dustin},
  year      = 2019,
  title     = {Measuring Calibration in Deep Learning.},
  booktitle = {CVPR Workshops},
  volume    = 2,
  number    = 7
}
@inproceedings{alibi,
  author    = {Ofir Press and Noah Smith and Mike Lewis},
  year      = 2022,
  title     = {Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
  booktitle = {International Conference on Learning Representations},
  url       = {https://openreview.net/forum?id=R8sQPpGCv0}
}
@article{pang2021quality,
  author  = {Pang, Richard Yuanzhe and Parrish, Alicia and Joshi, Nitish and Nangia, Nikita and Phang, Jason and Chen, Angelica and Padmakumar, Vishakh and Ma, Johnny and Thompson, Jana and He, He and others},
  year    = 2021,
  title   = {QuALITY: Question Answering with Long Input Texts, Yes!},
  journal = {arXiv preprint arXiv:2112.08608}
}
@article{paolini2021structured,
  author  = {Paolini, Giovanni and Athiwaratkun, Ben and Krone, Jason and Ma, Jie and Achille, Alessandro and Anubhai, Rishita and Santos, Cicero Nogueira dos and Xiang, Bing and Soatto, Stefano},
  year    = 2021,
  title   = {Structured prediction as translation between augmented natural languages},
  journal = {arXiv preprint arXiv:2101.05779}
}
@inproceedings{pappas-etal-2020-biomrc,
  author    = {Pappas, Dimitris  and Stavropoulos, Petros  and Androutsopoulos, Ion  and McDonald, Ryan},
  year      = 2020,
  month     = jul,
  title     = {{B}io{MRC}: A Dataset for Biomedical Machine Reading Comprehension},
  booktitle = {Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing},
  publisher = {Association for Computational Linguistics},
  address   = {Online},
  pages     = {140--149},
  doi       = {10.18653/v1/2020.bionlp-1.15},
  url       = {https://aclanthology.org/2020.bionlp-1.15},
  abstract  = {We introduceBIOMRC, a large-scale cloze-style biomedical MRC dataset. Care was taken to reduce noise, compared to the previous BIOREAD dataset of Pappas et al. (2018). Experiments show that simple heuristics do not perform well on the new dataset and that two neural MRC models that had been tested on BIOREAD perform much better on BIOMRC, indicating that the new dataset is indeed less noisy or at least that its task is more feasible. Non-expert human performance is also higher on the new dataset compared to BIOREAD, and biomedical experts perform even better. We also introduce a new BERT-based MRC model, the best version of which substantially outperforms all other methods tested, reaching or surpassing the accuracy of biomedical experts in some experiments. We make the new dataset available in three different sizes, also releasing our code, and providing a leaderboard.}
}
@misc{https://doi.org/10.48550/arxiv.2004.10796,
  author    = {Park, Jae Sung and Bhagavatula, Chandra and Mottaghi, Roozbeh and Farhadi, Ali and Choi, Yejin},
  year      = 2020,
  title     = {VisualCOMET: Reasoning about the Dynamic Context of a Still Image},
  publisher = {arXiv},
  doi       = {10.48550/ARXIV.2004.10796},
  url       = {https://arxiv.org/abs/2004.10796},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@inproceedings{park2019cord,
  author    = {Park, Seunghyun and Shin, Seung and Lee, Bado and Lee, Junyeop and Surh, Jaeheung and Seo, Minjoon and Lee, Hwalsuk},
  year      = 2019,
  title     = {CORD: a consolidated receipt dataset for post-OCR parsing},
  booktitle = {Workshop on Document Intelligence at NeurIPS 2019}
}
@inproceedings{park2019cord,
  author    = {Park, Seunghyun and Shin, Seung and Lee, Bado and Lee, Junyeop and Surh, Jaeheung and Seo, Minjoon and Lee, Hwalsuk},
  year      = 2019,
  title     = {CORD: a consolidated receipt dataset for post-OCR parsing},
  booktitle = {Workshop on Document Intelligence at NeurIPS 2019}
}
@article{pasupat2015compositional,
  author  = {Pasupat, Panupong and Liang, Percy},
  year    = 2015,
  title   = {Compositional semantic parsing on semi-structured tables},
  journal = {arXiv preprint arXiv:1508.00305}
}
@inproceedings{jaeger2023a,
  author    = {Paul F Jaeger and Carsten Tim L{\"u}th and Lukas Klein and Till J. Bungert},
  year      = 2023,
  title     = {A Call to Reflect on Evaluation Practices for Failure Detection in Image Classification},
  booktitle = {International Conference on Learning Representations},
  url       = {https://openreview.net/forum?id=YnkGMIh0gvX}
}
@inproceedings{paz-argaman-tsarfaty-2019-run,
  author    = {Paz-Argaman, Tzuf  and Tsarfaty, Reut},
  year      = 2019,
  month     = nov,
  title     = {{RUN} through the Streets: A New Dataset and Baseline Models for Realistic Urban Navigation},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  publisher = {Association for Computational Linguistics},
  address   = {Hong Kong, China},
  pages     = {6449--6455},
  doi       = {10.18653/v1/D19-1681},
  url       = {https://aclanthology.org/D19-1681},
  abstract  = {Following navigation instructions in natural language (NL) requires a composition of language, action, and knowledge of the environment. Knowledge of the environment may be provided via visual sensors or as a symbolic world representation referred to as a map. Previous work on map-based NL navigation relied on small artificial worlds with a fixed set of entities known in advance. Here we introduce the Realistic Urban Navigation (RUN) task, aimed at interpreting NL navigation instructions based on a real, dense, urban map. Using Amazon Mechanical Turk, we collected a dataset of 2515 instructions aligned with actual routes over three regions of Manhattan. We then empirically study which aspects of a neural architecture are important for the RUN success, and empirically show that entity abstraction, attention over words and worlds, and a constantly updating world-state, significantly contribute to task accuracy.}
}
@inproceedings{pfitzmann2022doclaynet,
  author    = {Pfitzmann, Birgit and Auer, Christoph and Dolfi, Michele and Nassar, Ahmed S and Staar, Peter},
  year      = 2022,
  title     = {DocLayNet: A Large Human-Annotated Dataset for Document-Layout Segmentation},
  booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages     = {3743--3751}
}
@misc{https://doi.org/10.48550/arxiv.2206.04045,
  author    = {Pietruszka, Michał and Turski, Michał and Borchmann, Łukasz and Dwojak, Tomasz and Pałka, Gabriela and Szyndler, Karolina and Jurkiewicz, Dawid and Garncarek, Łukasz},
  year      = 2022,
  title     = {STable: Table Generation Framework for Encoder-Decoder Models},
  publisher = {arXiv},
  doi       = {10.48550/ARXIV.2206.04045},
  url       = {https://arxiv.org/abs/2206.04045},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
  keywords  = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@article{pilan2022text,
  author    = {Pil{\'a}n, Ildik{\'o} and Lison, Pierre and {\O}vrelid, Lilja and Papadopoulou, Anthi and S{\'a}nchez, David and Batet, Montserrat},
  year      = 2022,
  title     = {The text anonymization benchmark (tab): A dedicated corpus and evaluation framework for text anonymization},
  journal   = {Computational Linguistics},
  publisher = {MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…},
  volume    = 48,
  number    = 4,
  pages     = {1053--1101}
}
@article{pramanik2020towards,
  author  = {Pramanik, Subhojeet and Mujumdar, Shashank and Patel, Hima},
  year    = 2020,
  title   = {Towards a multi-modal, multi-task learning based pre-training framework for document representation learning},
  journal = {arXiv preprint arXiv:2009.14457}
}
@inproceedings{qi-etal-2022-dureadervis,
  author    = {Qi, Le  and Lv, Shangwen  and Li, Hongyu  and Liu, Jing  and Zhang, Yu  and She, Qiaoqiao  and Wu, Hua  and Wang, Haifeng  and Liu, Ting},
  year      = 2022,
  month     = may,
  title     = {$\textrm{DuReader}_{\textrm{vis}}$: A {C}hinese Dataset for Open-domain Document Visual Question Answering},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2022},
  publisher = {Association for Computational Linguistics},
  address   = {Dublin, Ireland},
  pages     = {1338--1351},
  doi       = {10.18653/v1/2022.findings-acl.105},
  url       = {https://aclanthology.org/2022.findings-acl.105}
}
@inproceedings{qian-etal-2021-structural,
  author    = {Qian, Peng  and Naseem, Tahira  and Levy, Roger  and Fernandez Astudillo, Ram{\'o}n},
  year      = 2021,
  month     = aug,
  title     = {Structural Guidance for Transformer Language Models},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  publisher = {Association for Computational Linguistics},
  address   = {Online},
  pages     = {3735--3745},
  doi       = {10.18653/v1/2021.acl-long.289},
  url       = {https://aclanthology.org/2021.acl-long.289}
}
@article{qin2022t5score,
  author  = {Qin, Yiwei and Yuan, Weizhe and Neubig, Graham and Liu, Pengfei},
  year    = 2022,
  title   = {T5Score: Discriminative Fine-tuning of Generative Evaluation Metrics},
  journal = {arXiv preprint arXiv:2212.05726}
}
@inproceedings{quinonero2005evaluating,
  author       = {Quinonero-Candela, Joaquin and Rasmussen, Carl Edward and Sinz, Fabian and Bousquet, Olivier and Sch{\"o}lkopf, Bernhard},
  year         = 2005,
  title        = {Evaluating predictive uncertainty challenge},
  booktitle    = {Machine Learning Challenges Workshop},
  pages        = {1--27},
  organization = {Springer}
}
@inproceedings{quinonero2005evaluating,
  author       = {Quinonero-Candela, Joaquin and Rasmussen, Carl Edward and Sinz, Fabian and Bousquet, Olivier and Sch{\"o}lkopf, Bernhard},
  year         = 2005,
  title        = {Evaluating predictive uncertainty challenge},
  booktitle    = {Machine Learning Challenges Workshop},
  pages        = {1--27},
  organization = {Springer}
}
@inproceedings{Powalski2021GoingFB,
  author    = {Rafal Powalski and Łukasz Borchmann and Dawid Jurkiewicz and Tomasz Dwojak and Michal Pietruszka and Gabriela Pałka},
  year      = 2021,
  title     = {Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer},
  booktitle = {ICDAR}
}
@article{raffel2020exploring,
  author  = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J and others},
  year    = 2020,
  title   = {Exploring the limits of transfer learning with a unified text-to-text transformer.},
  journal = {J. Mach. Learn. Res.},
  volume  = 21,
  number  = 140,
  pages   = {1--67}
}
@inproceedings{raghavan-etal-2021-emrkbqa,
  author    = {Raghavan, Preethi  and Liang, Jennifer J  and Mahajan, Diwakar  and Chandra, Rachita  and Szolovits, Peter},
  year      = 2021,
  month     = jun,
  title     = {emr{KBQA}: A Clinical Knowledge-Base Question Answering Dataset},
  booktitle = {Proceedings of the 20th Workshop on Biomedical Language Processing},
  publisher = {Association for Computational Linguistics},
  address   = {Online},
  pages     = {64--73},
  doi       = {10.18653/v1/2021.bionlp-1.7},
  url       = {https://aclanthology.org/2021.bionlp-1.7},
  abstract  = {We present emrKBQA, a dataset for answering physician questions from a structured patient record. It consists of questions, logical forms and answers. The questions and logical forms are generated based on real-world physician questions and are slot-filled and answered from patients in the MIMIC-III KB through a semi-automated process. This community-shared release consists of over 940000 question, logical form and answer triplets with 389 types of questions and {\textasciitilde}7.5 paraphrases per question type. We perform experiments to validate the quality of the dataset and set benchmarks for question to logical form learning that helps answer questions on this dataset.}
}
@inproceedings{rajpurkar-etal-2018-know,
  author    = {Rajpurkar, Pranav  and Jia, Robin  and Liang, Percy},
  year      = 2018,
  month     = jul,
  title     = {Know What You Don{'}t Know: Unanswerable Questions for {SQ}u{AD}},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  publisher = {Association for Computational Linguistics},
  address   = {Melbourne, Australia},
  pages     = {784--789},
  doi       = {10.18653/v1/P18-2124},
  url       = {https://aclanthology.org/P18-2124},
  abstract  = {Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86{\%} F1 on SQuAD achieves only 66{\%} F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD.}
}
@article{rajpurkar2016squad,
  author  = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  year    = 2016,
  title   = {Squad: 100,000+ questions for machine comprehension of text},
  journal = {arXiv preprint arXiv:1606.05250}
}
@inproceedings{roelofs2022mitigating,
  author       = {Roelofs, Rebecca and Cain, Nicholas and Shlens, Jonathon and Mozer, Michael C},
  year         = 2022,
  title        = {Mitigating bias in calibration error estimation},
  booktitle    = {International Conference on Artificial Intelligence and Statistics},
  pages        = {4036--4054},
  organization = {PMLR}
}
@article{rothe2020leveraging,
  author    = {Rothe, Sascha and Narayan, Shashi and Severyn, Aliaksei},
  year      = 2020,
  title     = {Leveraging pre-trained checkpoints for sequence generation tasks},
  journal   = {Transactions of the Association for Computational Linguistics},
  publisher = {MIT Press},
  volume    = 8,
  pages     = {264--280}
}
@article{russakovsky2015imagenet,
  author    = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  year      = 2015,
  title     = {Imagenet large scale visual recognition challenge},
  journal   = {International journal of computer vision},
  publisher = {Springer},
  volume    = 115,
  pages     = {211--252}
}
@inproceedings{VisualMRC2021,
  author    = {Ryota Tanaka and Kyosuke Nishida and Sen Yoshida},
  year      = 2021,
  title     = {VisualMRC: Machine Reading Comprehension on Document Images},
  booktitle = {AAAI}
}
@inproceedings{sage2021data,
  author       = {Sage, Cl{\'e}ment and Douzon, Thibault and Aussem, Alex and Eglin, V{\'e}ronique and Elghazel, Haytham and Duffner, Stefan and Garcia, Christophe and Espinas, J{\'e}r{\'e}my},
  year         = 2021,
  title        = {Data-efficient information extraction from documents with pre-trained language models},
  booktitle    = {Document Analysis and Recognition--ICDAR 2021 Workshops: Lausanne, Switzerland, September 5--10, 2021, Proceedings, Part II 16},
  pages        = {455--469},
  organization = {Springer}
}
@inproceedings{saxena-etal-2021-question,
  author    = {Saxena, Apoorv  and Chakrabarti, Soumen  and Talukdar, Partha},
  year      = 2021,
  month     = aug,
  title     = {Question Answering Over Temporal Knowledge Graphs},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  publisher = {Association for Computational Linguistics},
  address   = {Online},
  pages     = {6663--6676},
  doi       = {10.18653/v1/2021.acl-long.520},
  url       = {https://aclanthology.org/2021.acl-long.520},
  abstract  = {Temporal Knowledge Graphs (Temporal KGs) extend regular Knowledge Graphs by providing temporal scopes (start and end times) on each edge in the KG. While Question Answering over KG (KGQA) has received some attention from the research community, QA over Temporal KGs (Temporal KGQA) is a relatively unexplored area. Lack of broad coverage datasets has been another factor limiting progress in this area. We address this challenge by presenting CRONQUESTIONS, the largest known Temporal KGQA dataset, clearly stratified into buckets of structural complexity. CRONQUESTIONS expands the only known previous dataset by a factor of 340x. We find that various state-of-the-art KGQA methods fall far short of the desired performance on this new dataset. In response, we also propose CRONKGQA, a transformer-based solution that exploits recent advances in Temporal KG embeddings, and achieves performance superior to all baselines, with an increase of 120{\%} in accuracy over the next best performing method. Through extensive experiments, we give detailed insights into the workings of CRONKGQA, as well as situations where significant further improvements appear possible. In addition to the dataset, we have released our code as well.}
}
@inproceedings{shen2020large,
  author    = {Shen, Zejiang and Zhang, Kaixuan and Dell, Melissa},
  year      = 2020,
  title     = {A large dataset of historical Japanese documents with complex layouts},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  pages     = {548--549}
}
@inproceedings{shi2023sequence,
  author       = {Shi, Jiaxin and Wang, Ke Alexander and Fox, Emily},
  year         = 2023,
  title        = {Sequence Modeling with Multiresolution Convolutional Memory},
  booktitle    = {International Conference on Machine Learning},
  pages        = {31312--31327},
  organization = {PMLR}
}
@misc{geng2021romebert,
  author        = {Shijie Geng and Peng Gao and Zuohui Fu and Yongfeng Zhang},
  year          = 2021,
  title         = {RomeBERT: Robust Training of Multi-Exit BERT},
  eprint        = {2101.09755},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@inproceedings{singh2019towards,
  author    = {Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus},
  year      = 2019,
  title     = {Towards vqa models that can read},
  booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages     = {8317--8326}
}
@article{slossberg2020calibration,
  author  = {Slossberg, Ron and Anschel, Oron and Markovitz, Amir and Litman, Ron and Aberdam, Aviad and Tsiper, Shahar and Mazor, Shai and Wu, Jon and Manmatha, R},
  year    = 2020,
  title   = {On Calibration of Scene-Text Recognition Models},
  journal = {arXiv preprint arXiv:2012.12643}
}
@inproceedings{smock2022pubtables,
  author    = {Smock, Brandon and Pesala, Rohith and Abraham, Robin},
  year      = 2022,
  title     = {PubTables-1M: Towards comprehensive table extraction from unstructured documents},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {4634--4642}
}
@inproceedings{10.1145/3340531.3412760,
  author    = {Souza Costa, Tarc\'{\i}sio and Gottschalk, Simon and Demidova, Elena},
  year      = 2020,
  title     = {Event-QA: A Dataset for Event-Centric Question Answering over Knowledge Graphs},
  booktitle = {Proceedings of the 29th ACM International Conference on Information \& Knowledge Management},
  location  = {Virtual Event, Ireland},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  series    = {CIKM '20},
  pages     = {3157–3164},
  doi       = {10.1145/3340531.3412760},
  isbn      = 9781450368599,
  url       = {https://doi.org/10.1145/3340531.3412760},
  abstract  = {Semantic Question Answering (QA) is a crucial technology to facilitate intuitive user access to semantic information stored in knowledge graphs. Whereas most of the existing QA systems and datasets focus on entity-centric questions, very little is known about these systems' performance in the context of events. As new event-centric knowledge graphs emerge, datasets for such questions gain importance. In this paper, we present the Event-QA dataset for answering event-centric questions over knowledge graphs. Event-QA contains 1000 semantic queries and the corresponding English, German and Portuguese verbalizations for EventKG - an event-centric knowledge graph with more than 970 thousand events.},
  numpages  = 8,
  keywords  = {knowledge graphs, dataset, events, question answering}
}
@article{stephenson2008two,
  author  = {Stephenson, David B and Coelho, Caio AS and Jolliffe, Ian T},
  year    = 2008,
  title   = {Two extra components in the Brier score decomposition},
  journal = {Weather and Forecasting},
  volume  = 23,
  number  = 4,
  pages   = {752--757}
}
@article{stephenson2008two,
  author  = {Stephenson, David B and Coelho, Caio AS and Jolliffe, Ian T},
  year    = 2008,
  title   = {Two extra components in the Brier score decomposition},
  journal = {Weather and Forecasting},
  volume  = 23,
  number  = 4,
  pages   = {752--757}
}
@misc{straydeepform,
  author = {Stray, J and Svetlichnaya, S},
  title  = {DeepForm: extract information from documents (2020)}
}
@misc{SlideVQA,
  author    = {Tanaka, Ryota and Nishida, Kyosuke and Nishida, Kosuke and Hasegawa, Taku and Saito, Itsumi and Saito, Kuniko},
  year      = 2023,
  title     = {SlideVQA: A Dataset for Document Visual Question Answering on Multiple Images},
  publisher = {arXiv},
  doi       = {10.48550/ARXIV.2301.04883},
  url       = {https://arxiv.org/abs/2301.04883},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords  = {Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@inproceedings{tang2023unifying,
  author    = {Tang, Zineng and Yang, Ziyi and Wang, Guoxin and Fang, Yuwei and Liu, Yang and Zhu, Chenguang and Zeng, Michael and Zhang, Cha and Bansal, Mohit},
  year      = 2023,
  title     = {Unifying vision, text, and layout for universal document processing},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {19254--19264}
}
@inproceedings{thorne-etal-2018-fever,
  author    = {Thorne, James  and Vlachos, Andreas  and Christodoulopoulos, Christos  and Mittal, Arpit},
  year      = 2018,
  month     = jun,
  title     = {{FEVER}: a Large-scale Dataset for Fact Extraction and {VER}ification},
  booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
  publisher = {Association for Computational Linguistics},
  address   = {New Orleans, Louisiana},
  pages     = {809--819},
  doi       = {10.18653/v1/N18-1074},
  url       = {https://aclanthology.org/N18-1074},
  abstract  = {In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss kappa. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87{\%}, while if we ignore the evidence we achieve 50.91{\%}. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.}
}
@inproceedings{tito2021document,
  author       = {Tito, Rub{\`e}n and Karatzas, Dimosthenis and Valveny, Ernest},
  year         = 2021,
  title        = {Document collection visual question answering},
  booktitle    = {Document Analysis and Recognition--ICDAR 2021: 16th International Conference, Lausanne, Switzerland, September 5--10, 2021, Proceedings, Part II 16},
  pages        = {778--792},
  organization = {Springer}
}
@article{tito2022hierarchical,
  author  = {Tito, Rub{\`e}n and Karatzas, Dimosthenis and Valveny, Ernest},
  year    = 2022,
  title   = {Hierarchical multimodal transformers for Multi-Page DocVQA},
  journal = {arXiv preprint arXiv:2212.05935}
}
@inproceedings{tito2021icdar,
  author       = {Tito, Rub{\`e}n and Mathew, Minesh and Jawahar, CV and Valveny, Ernest and Karatzas, Dimosthenis},
  year         = 2021,
  title        = {Icdar 2021 competition on document visual question answering},
  booktitle    = {International Conference on Document Analysis and Recognition},
  pages        = {635--649},
  organization = {Springer}
}
@inproceedings{kleisterStanislawekGWLK21,
  author    = {Tomasz Stanislawek and Filip Gralinski and Anna Wr{\'{o}}blewska and Dawid Lipinski and Agnieszka Kaliska and Paulina Rosalska and Bartosz Topolski and Przemyslaw Biecek},
  year      = 2021,
  title     = {Kleister: Key Information Extraction Datasets Involving Long Documents with Complex Layouts},
  booktitle = {ICDAR},
  publisher = {Springer},
  series    = {Lecture Notes in Computer Science},
  volume    = 12821,
  pages     = {564--579},
  doi       = {10.1007/978-3-030-86549-8\_36}
}
@article{trischler2016newsqa,
  author  = {Trischler, Adam and Wang, Tong and Yuan, Xingdi and Harris, Justin and Sordoni, Alessandro and Bachman, Philip and Suleman, Kaheer},
  year    = 2016,
  title   = {Newsqa: A machine comprehension dataset},
  journal = {arXiv preprint arXiv:1611.09830}
}
@inproceedings{trivedi2017lc,
  author       = {Trivedi, Priyansh and Maheshwari, Gaurav and Dubey, Mohnish and Lehmann, Jens},
  year         = 2017,
  title        = {Lc-quad: A corpus for complex question answering over knowledge graphs},
  booktitle    = {International Semantic Web Conference},
  pages        = {210--218},
  organization = {Springer}
}
@article{turski2023ccpdf,
  author  = {Turski, Micha{\l} and Stanis{\l}awek, Tomasz and Kaczmarek, Karol and Dyda, Pawe{\l} and Grali{\'n}ski, Filip},
  year    = 2023,
  title   = {CCpdf: Building a High Quality Corpus for Visually Rich Documents from Web Crawl Data},
  journal = {arXiv preprint arXiv:2304.14953}
}
@inproceedings{vaicenavicius2019evaluating,
  author       = {Vaicenavicius, Juozas and Widmann, David and Andersson, Carl and Lindsten, Fredrik and Roll, Jacob and Sch{\"o}n, Thomas},
  year         = 2019,
  title        = {Evaluating model calibration in classification},
  booktitle    = {The 22nd International Conference on Artificial Intelligence and Statistics},
  pages        = {3459--3467},
  organization = {PMLR}
}
@inproceedings{vaicenavicius2019evaluating,
  author       = {Vaicenavicius, Juozas and Widmann, David and Andersson, Carl and Lindsten, Fredrik and Roll, Jacob and Sch{\"o}n, Thomas},
  year         = 2019,
  title        = {Evaluating model calibration in classification},
  booktitle    = {The 22nd International Conference on Artificial Intelligence and Statistics},
  pages        = {3459--3467},
  organization = {PMLR}
}
@article{JVLUQuant,
  author  = {Van Landeghem, Jordy and Blaschko, Matthew and Anckaert, Bertrand and Moens, Marie-Francine},
  year    = 2022,
  title   = {Benchmarking Scalable Predictive Uncertainty in Text Classification},
  journal = {IEEE Access},
  volume  = 10,
  pages   = {43703--43737}
}
%% qa specific
@inproceedings{dude2023icdar,
  author    = {Van Landeghem, Jordy and Borchmann, Lukasz and  Tito, Rubèn and  Pietruszka, Michał and Jurkiewicz, Dawid  and  Powalski, Rafał and Józiak, Paweł and Biswas, Sanket and Coustaty, Mickaël and  Stanisławek, Tomasz},
  year      = 2023,
  title     = {{ICDAR 2023 Competition on Document UnderstanDing of Everything (DUDE)}},
  booktitle = {Proceedings of  ICDAR 2023}
}
@inproceedings{vanlandeghem2023document,
  author    = {Van Landeghem, Jordy and Rub\`{e}n Tito and {\L}ukasz Borchmann and Micha{\l} Pietruszka and Pawel Joziak and Rafal Powalski and Dawid Jurkiewicz and Mickael Coustaty and Bertrand Ackaert and Ernest Valveny and Matthew B. Blaschko and Marie-Francine Moens and Tomasz Stanislawek},
  year      = 2023,
  title     = {{Document Understanding Dataset and Evaluation (DUDE)}},
  booktitle = {International Conference on Computer Vision}
}
@inproceedings{vapnik1992principles,
  author    = {Vapnik, Vladimir},
  year      = 1992,
  title     = {Principles of risk minimization for learning theory},
  booktitle = {Advances in neural information processing systems},
  pages     = {831--838}
}
@article{vaswani2017attention,
  author  = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year    = 2017,
  title   = {Attention is all you need},
  journal = {Advances in neural information processing systems},
  volume  = 30
}
@article{wang2022benchmark,
  author  = {Wang, Zilong and Zhou, Yichao and Wei, Wei and Lee, Chen-Yu and Tata, Sandeep},
  year    = 2022,
  title   = {A Benchmark for Structured Extractions from Complex Documents},
  journal = {arXiv preprint arXiv:2211.15421}
}
@article{wei2022emergent,
  author  = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  year    = 2022,
  title   = {Emergent abilities of large language models},
  journal = {arXiv preprint arXiv:2206.07682}
}
@article{welbl2018constructing,
  author  = {Welbl, Johannes and Stenetorp, Pontus and Riedel, Sebastian},
  year    = 2018,
  title   = {Constructing datasets for multi-hop reading comprehension across documents},
  journal = {Transactions of the Association for Computational Linguistics},
  volume  = 6,
  pages   = {287--302}
}
@article{wiedemann2021multi,
  author    = {Wiedemann, Gregor and Heyer, Gerhard},
  year      = 2021,
  title     = {Multi-modal page stream segmentation with convolutional neural networks},
  journal   = {Language Resources and Evaluation},
  publisher = {Springer},
  volume    = 55,
  pages     = {127--150}
}
@misc{zhang2022nail,
  author = {Xinbo Zhang and Changzhi Sun and Yue Zhang and Lei Li and Hao Zhou},
  year   = 2022,
  title  = {{NAIL}: A Challenging Benchmark for Na{\textbackslash}''ive Logical Reasoning},
  url    = {https://openreview.net/forum?id=djhu4DIZZHR}
}
@inproceedings{xu2020layoutlm,
  author    = {Xu, Yiheng and Li, Minghao and Cui, Lei and Huang, Shaohan and Wei, Furu and Zhou, Ming},
  year      = 2020,
  title     = {Layoutlm: Pre-training of text and layout for document image understanding},
  booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages     = {1192--1200}
}
@misc{https://doi.org/10.48550/arxiv.2204.07408,
  author    = {Yang, Linyi and Wang, Zhen and Wu, Yuxiang and Yang, Jie and Zhang, Yue},
  year      = 2022,
  title     = {Towards Fine-grained Causal Reasoning and QA},
  publisher = {arXiv},
  doi       = {10.48550/ARXIV.2204.07408},
  url       = {https://arxiv.org/abs/2204.07408},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
  keywords  = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Logic in Computer Science (cs.LO), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@inproceedings{yang-etal-2015-wikiqa,
  author    = {Yang, Yi  and Yih, Wen-tau  and Meek, Christopher},
  year      = 2015,
  month     = sep,
  title     = {{W}iki{QA}: A Challenge Dataset for Open-Domain Question Answering},
  booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  publisher = {Association for Computational Linguistics},
  address   = {Lisbon, Portugal},
  pages     = {2013--2018},
  doi       = {10.18653/v1/D15-1237},
  url       = {https://aclanthology.org/D15-1237}
}
@article{yang2022multi,
  author  = {Yang, Yuzhe and Wang, Hao and Katabi, Dina},
  year    = 2022,
  title   = {On Multi-Domain Long-Tailed Recognition, Generalization and Beyond},
  journal = {arXiv preprint arXiv:2203.09513}
}
@article{yang2018hotpotqa,
  author  = {Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W and Salakhutdinov, Ruslan and Manning, Christopher D},
  year    = 2018,
  title   = {HotpotQA: A dataset for diverse, explainable multi-hop question answering},
  journal = {arXiv preprint arXiv:1809.09600}
}
@article{ye2021pack,
  author  = {Ye, Deming and Lin, Yankai and Sun, Maosong},
  year    = 2021,
  title   = {Pack Together: Entity and Relation Extraction with Levitated Marker},
  journal = {arXiv preprint arXiv:2109.06067}
}
@inproceedings{yoshikawa-etal-2017-stair,
  author    = {Yoshikawa, Yuya  and Shigeto, Yutaro  and Takeuchi, Akikazu},
  year      = 2017,
  month     = jul,
  title     = {{STAIR} Captions: Constructing a Large-Scale {J}apanese Image Caption Dataset},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  publisher = {Association for Computational Linguistics},
  address   = {Vancouver, Canada},
  pages     = {417--421},
  doi       = {10.18653/v1/P17-2066},
  url       = {https://aclanthology.org/P17-2066},
  abstract  = {In recent years, automatic generation of image descriptions (captions), that is, image captioning, has attracted a great deal of attention. In this paper, we particularly consider generating Japanese captions for images. Since most available caption datasets have been constructed for English language, there are few datasets for Japanese. To tackle this problem, we construct a large-scale Japanese image caption dataset based on images from MS-COCO, which is called STAIR Captions. STAIR Captions consists of 820,310 Japanese captions for 164,062 images. In the experiment, we show that a neural network trained using STAIR Captions can generate more natural and better Japanese captions, compared to those generated using English-Japanese machine translation after generating English captions.}
}
@inproceedings{you-etal-2022-end,
  author    = {You, Chenyu  and Chen, Nuo  and Liu, Fenglin  and Ge, Shen  and Wu, Xian  and Zou, Yuexian},
  year      = 2022,
  month     = jul,
  title     = {End-to-end Spoken Conversational Question Answering: Task, Dataset and Model},
  booktitle = {Findings of the Association for Computational Linguistics: NAACL 2022},
  publisher = {Association for Computational Linguistics},
  address   = {Seattle, United States},
  pages     = {1219--1232},
  doi       = {10.18653/v1/2022.findings-naacl.91},
  url       = {https://aclanthology.org/2022.findings-naacl.91},
  abstract  = {In spoken question answering, the systems are designed to answer questions from contiguous text spans within the related speech transcripts. However, the most natural way that human seek or test their knowledge is via human conversations. Therefore, we propose a new Spoken Conversational Question Answering task (SCQA), aiming at enabling the systems to model complex dialogues flow given the speech documents. In this task, our main objective is to build the system to deal with conversational questions based on the audio recordings, and to explore the plausibility of providing more cues from different modalities with systems in information gathering. To this end, instead of directly adopting automatically generated speech transcripts with highly noisy data, we propose a novel unified data distillation approach, DDNet, which effectively ingests cross-modal information to achieve fine-grained representations of the speech and language modalities. Moreover, we propose a simple and novel mechanism, termed Dual Attention, by encouraging better alignments between audio and text to ease the process of knowledge transfer. To evaluate the capacity of SCQA systems in a dialogue-style interaction, we assemble a Spoken Conversational Question Answering (Spoken-CoQA) dataset with more than 40k question-answer pairs from 4k conversations. We first show that the performance of the existing state-of-the-art methods significantly degrade on our dataset, hence demonstrating the necessity of incorporating cross-modal information to achieve good performance gains. Our experimental results demonstrate that our proposed method achieves superior performance in spoken conversational question answering. Codes and datasets will be made publicly available.}
}
@inproceedings{yu2020reclor,
  author    = {Yu, Weihao and Jiang, Zihang and Dong, Yanfei and Feng, Jiashi},
  year      = 2020,
  month     = {April},
  title     = {ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning},
  booktitle = {International Conference on Learning Representations (ICLR)}
}
@inproceedings{yu2021pick,
  author       = {Yu, Wenwen and Lu, Ning and Qi, Xianbiao and Gong, Ping and Xiao, Rong},
  year         = 2021,
  title        = {Pick: Processing key information extraction from documents using improved graph learning-convolutional networks},
  booktitle    = {2020 25th International Conference on Pattern Recognition (ICPR)},
  pages        = {4363--4370},
  organization = {IEEE}
}
@inproceedings{yu2021pick,
  author       = {Yu, Wenwen and Lu, Ning and Qi, Xianbiao and Gong, Ping and Xiao, Rong},
  year         = 2021,
  title        = {Pick: Processing key information extraction from documents using improved graph learning-convolutional networks},
  booktitle    = {2020 25th International Conference on Pattern Recognition (ICPR)},
  pages        = {4363--4370},
  organization = {IEEE}
}
@inproceedings{yun2021re,
  author    = {Yun, Sangdoo and Oh, Seong Joon and Heo, Byeongho and Han, Dongyoon and Choe, Junsuk and Chun, Sanghyuk},
  year      = 2021,
  title     = {Re-labeling imagenet: from single to multi-labels, from global to localized labels},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {2340--2350}
}
@inproceedings{zadrozny2002transforming,
  author    = {Zadrozny, Bianca and Elkan, Charles},
  year      = 2002,
  title     = {Transforming classifier scores into accurate multiclass probability estimates},
  booktitle = {Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages     = {694--699}
}
% Big Bird
@article{zaheer2020big,
  author  = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  year    = 2020,
  title   = {Big bird: Transformers for longer sequences},
  journal = {Advances in Neural Information Processing Systems},
  volume  = 33,
  pages   = {17283--17297}
}
@inproceedings{zarharan-etal-2021-parsfever,
  author    = {Zarharan, Majid  and Ghaderan, Mahsa  and Pourdabiri, Amin  and Sayedi, Zahra  and Minaei-Bidgoli, Behrouz  and Eetemadi, Sauleh  and Pilehvar, Mohammad Taher},
  year      = 2021,
  month     = aug,
  title     = {{P}ars{FEVER}: a Dataset for {F}arsi Fact Extraction and Verification},
  booktitle = {Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics},
  publisher = {Association for Computational Linguistics},
  address   = {Online},
  pages     = {99--104},
  doi       = {10.18653/v1/2021.starsem-1.9},
  url       = {https://aclanthology.org/2021.starsem-1.9},
  abstract  = {Training and evaluation of automatic fact extraction and verification techniques require large amounts of annotated data which might not be available for low-resource languages. This paper presents ParsFEVER: the first publicly available Farsi dataset for fact extraction and verification. We adopt the construction procedure of the standard English dataset for the task, i.e., FEVER, and improve it for the case of low-resource languages. Specifically, claims are extracted from sentences that are carefully selected to be more informative. The dataset comprises nearly 23K manually-annotated claims. Over 65{\%} of the claims in ParsFEVER are many-hop (require evidence from multiple sources), making the dataset a challenging benchmark (only 13{\%} of the claims in FEVER are many-hop). Also, despite having a smaller training set (around one-ninth of that in Fever), a model trained on ParsFEVER attains similar downstream performance, indicating the quality of the dataset. We release the dataset and the annotation guidelines at https://github.com/Zarharan/ParsFEVER.}
}
@inproceedings{zhang-etal-2021-noahqa-numerical,
  author    = {Zhang, Qiyuan  and Wang, Lei  and Yu, Sicheng  and Wang, Shuohang  and Wang, Yang  and Jiang, Jing  and Lim, Ee-Peng},
  year      = 2021,
  month     = nov,
  title     = {{NOAHQA}: Numerical Reasoning with Interpretable Graph Question Answering Dataset},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2021},
  publisher = {Association for Computational Linguistics},
  address   = {Punta Cana, Dominican Republic},
  pages     = {4147--4161},
  doi       = {10.18653/v1/2021.findings-emnlp.350},
  url       = {https://aclanthology.org/2021.findings-emnlp.350},
  abstract  = {While diverse question answering (QA) datasets have been proposed and contributed significantly to the development of deep learning models for QA tasks, the existing datasets fall short in two aspects. First, we lack QA datasets covering complex questions that involve answers as well as the reasoning processes to get them. As a result, the state-of-the-art QA research on numerical reasoning still focuses on simple calculations and does not provide the mathematical expressions or evidence justifying the answers. Second, the QA community has contributed a lot of effort to improve the interpretability of QA models. However, they fail to explicitly show the reasoning process, such as the evidence order for reasoning and the interactions between different pieces of evidence. To address the above shortcoming, we introduce NOAHQA, a conversational and bilingual QA dataset with questions requiring numerical reasoning with compound mathematical expressions. With NOAHQA, we develop an interpretable reasoning graph as well as the appropriate evaluation metric to measure the answer quality. We evaluate the state-of-the-art QA models trained using existing QA datasets on NOAHQA and show that the best among them can only achieve 55.5 exact match scores, while the human performance is 89.7. We also present a new QA model for generating a reasoning graph where the reasoning graph metric still has a large gap compared with that of humans, eg, 28 scores.}
}
@article{zhang2021knowing,
  author  = {Zhang, Shujian and Gong, Chengyue and Choi, Eunsol},
  year    = 2021,
  title   = {Knowing More About Questions Can Help: Improving Calibration in Question Answering},
  journal = {arXiv preprint arXiv:2106.01494}
}
@inproceedings{zheng2021global,
  author    = {Zheng, Xinyi and Burdick, Douglas and Popa, Lucian and Zhong, Xu and Wang, Nancy Xin Ru},
  year      = 2021,
  title     = {Global table extractor (gte): A framework for joint table identification and cell structure recognition using visual context},
  booktitle = {Proceedings of the IEEE/CVF winter conference on applications of computer vision},
  pages     = {697--706}
}
@inproceedings{zhong2020image,
  author       = {Zhong, Xu and ShafieiBavani, Elaheh and Jimeno Yepes, Antonio},
  year         = 2020,
  title        = {Image-based table recognition: data, model, and evaluation},
  booktitle    = {Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XXI 16},
  pages        = {564--580},
  organization = {Springer}
}
@inproceedings{zhong2019publaynet,
  author       = {Zhong, Xu and Tang, Jianbin and Yepes, Antonio Jimeno},
  year         = 2019,
  title        = {Publaynet: largest dataset ever for document layout analysis},
  booktitle    = {2019 International Conference on Document Analysis and Recognition (ICDAR)},
  pages        = {1015--1022},
  organization = {IEEE}
}
@inproceedings{zhu2022towards,
  author    = {Zhu, Fengbin and Lei, Wenqiang and Feng, Fuli and Wang, Chao and Zhang, Haozhou and Chua, Tat-Seng},
  year      = 2022,
  title     = {Towards complex document understanding by discrete reasoning},
  booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
  pages     = {4857--4866}
}
@inproceedings{zhu2007automatic,
  author       = {Zhu, Guangyu and Doermann, David},
  year         = 2007,
  title        = {Automatic document logo detection},
  booktitle    = {Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)},
  volume       = 2,
  pages        = {864--868},
  organization = {IEEE}
}

@misc{vanlandeghem2023document,
      title={Beyond Document Page Classification: Design, Datasets, and 
Challenges}, 
      author={Jordy Van Landeghem and Sanket Biswas and Matthew B. 
Blaschko and Marie-Francine Moens},
      year={2023},
      eprint={2308.12896},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{lin2022scrib,
  title={SCRIB: set-classifier with class-specific risk bounds for blackbox models},
  author={Lin, Zhen and Glass, Lucas and Westover, M Brandon and Xiao, Cao and Sun, Jimeng},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={7},
  pages={7497--7505},
  year={2022}
}

@inproceedings{geifman2018bias,
  title={Bias-Reduced Uncertainty Estimation for Deep Neural Classifiers},
  author={Geifman, Yonatan and Uziel, Guy and El-Yaniv, Ran},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{ni2019calibration,
  title={On the calibration of multiclass classification with rejection},
  author={Ni, Chenri and Charoenphakdee, Nontawat and Honda, Junya and Sugiyama, Masashi},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{zhu2022rethinking,
	title={Rethinking Confidence Calibration for Failure Prediction},
	author={Zhu, Fei and Cheng, Zhen and Zhang, Xu-Yao and Liu, Cheng-Lin},
	booktitle={Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XXV},
	pages={518--536},
	year={2022},
	organization={Springer}
}

@article{blasiok2023does,
	title={When Does Optimizing a Proper Loss Yield Calibration?},
	author={B{\l}asiok, Jaros{\l}aw and Gopalan, Parikshit and Hu, Lunjia and Nakkiran, Preetum},
	journal={arXiv preprint arXiv:2305.18764},
	year={2023}
}

@inproceedings{galil2023what,
	title={What Can we Learn From The Selective Prediction And Uncertainty Estimation Performance Of 523 Imagenet Classifiers?},
	author={Ido Galil and Mohammed Dabbah and Ran El-Yaniv},
	booktitle={The Eleventh International Conference on Learning Representations },
	year={2023},
	url={https://openreview.net/forum?id=p66AzKi6Xim}
}

@inproceedings{wei2022mitigating,
	title={Mitigating neural network overconfidence with logit normalization},
	author={Wei, Hongxin and Xie, Renchunzi and Cheng, Hao and Feng, Lei and An, Bo and Li, Yixuan},
	booktitle={International Conference on Machine Learning},
	pages={23631--23644},
	year={2022},
	organization={PMLR}
}

@inproceedings{guptatop,
	title={Top-label calibration and multiclass-to-binary reductions},
	author={Gupta, Chirag and Ramdas, Aaditya},
	booktitle={International Conference on Learning Representations}, 
	year={2022}
}

@inproceedings{zhu2023openmix,
  title={OpenMix: Exploring Outlier Samples for Misclassification Detection},
  author={Zhu, Fei and Cheng, Zhen and Zhang, Xu-Yao and Liu, Cheng-Lin},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12074--12083},
  year={2023}
}

@inproceedings{feng2023towards,
    title={Towards Better Selective Classification},
    author={Leo Feng and Mohamed Osama Ahmed and Hossein Hajimirsadeghi and Amir H. Abdi},
    booktitle={International Conference on Learning Representations},
    year={2023},
    url={https://openreview.net/forum?id=5gDz_yTcst}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@inproceedings{wei2022mitigating,
  title={Mitigating neural network overconfidence with logit normalization},
  author={Wei, Hongxin and Xie, Renchunzi and Cheng, Hao and Feng, Lei and An, Bo and Li, Yixuan},
  booktitle={International Conference on Machine Learning},
  pages={23631--23644},
  year={2022},
  organization={PMLR}
}

@article{liu2019deep,
  title={Deep gamblers: Learning to abstain with portfolio theory},
  author={Liu, Ziyin and Wang, Zhikang and Liang, Paul Pu and Salakhutdinov, Russ R and Morency, Louis-Philippe and Ueda, Masahito},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{wang2019symmetric,
  title={Symmetric cross entropy for robust learning with noisy labels},
  author={Wang, Yisen and Ma, Xingjun and Chen, Zaiyi and Luo, Yuan and Yi, Jinfeng and Bailey, James},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={322--330},
  year={2019}
}


@article{gou2021knowledge,
  title={Knowledge distillation: A survey},
  author={Gou, Jianping and Yu, Baosheng and Maybank, Stephen J and Tao, Dacheng},
  journal={International Journal of Computer Vision},
  volume={129},
  pages={1789--1819},
  year={2021},
  publisher={Springer}
}

@article{wang2022efficient,
  title={Efficient knowledge distillation from model checkpoints},
  author={Wang, Chaofei and Yang, Qisen and Huang, Rui and Song, Shiji and Huang, Gao},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={607--619},
  year={2022}
}

@inproceedings{moon2020confidence,
  title={Confidence-aware learning for deep neural networks},
  author={Moon, Jooyoung and Kim, Jihyo and Shin, Younghak and Hwang, Sangheum},
  booktitle={international conference on machine learning},
  pages={7034--7044},
  year={2020},
  organization={PMLR}
}

@article{feng2022stop,
  title={Stop overcomplicating selective classification: Use max-logit},
  author={Feng, Leo and Ahmed, Mohamed Osama and Hajimirsadeghi, Hossein and Abdi, Amir},
  journal={arXiv preprint arXiv:2206.09034},
  year={2022}
}

@inproceedings{zhang2020distilling,
  title={Distilling effective supervision from severe label noise},
  author={Zhang, Zizhao and Zhang, Han and Arik, Sercan O and Lee, Honglak and Pfister, Tomas},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9294--9303},
  year={2020}
}

@article{huang2020self,
  title={Self-adaptive training: beyond empirical risk minimization},
  author={Huang, Lang and Zhang, Chao and Zhang, Hongyang},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={19365--19376},
  year={2020}
}

@misc{
gomes2023a,
title={A simple Training-Free Method for Rejection Option},
author={Eduardo Dadalto C{\^a}mara Gomes and Marco Romanelli and Federica Granese and Pablo Piantanida},
year={2023},
url={https://openreview.net/forum?id=K1DdnjL6p7}
}

@article{granese2021doctor,
  title={Doctor: A simple method for detecting misclassification errors},
  author={Granese, Federica and Romanelli, Marco and Gorla, Daniele and Palamidessi, Catuscia and Piantanida, Pablo},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={5669--5681},
  year={2021}
}

@article{fisch2022calibrated,
  title={Calibrated selective classification},
  author={Fisch, Adam and Jaakkola, Tommi and Barzilay, Regina},
  journal={arXiv preprint arXiv:2208.12084},
  year={2022}
}

%Distillation methods
@INPROCEEDINGS {SimKD,
author = {D. Chen and J. Mei and H. Zhang and C. Wang and Y. Feng and C. Chen},
booktitle = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {Knowledge Distillation with the Reused Teacher Classifier},
year = {2022},
publisher = {IEEE Computer Society},
}


@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@article{ba2014deep,
  title={Do deep nets really need to be deep?},
  author={Ba, Jimmy and Caruana, Rich},
  journal={Advances in neural information processing systems},
  year={2014}
}

@article{romero2014fitnets,
  title={Fitnets: Hints for thin deep nets},
  author={Romero, Adriana and Ballas, Nicolas and Kahou, Samira Ebrahimi and Chassang, Antoine and Gatta, Carlo and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1412.6550},
  year={2014}
}

@inproceedings{chen2021distilling,
  title={Distilling knowledge via knowledge review},
  author={Chen, Pengguang and Liu, Shu and Zhao, Hengshuang and Jia, Jiaya},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2021}
}

@inproceedings{tian2019contrastive,
  title={Contrastive representation distillation},
  author={Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2019}
}

@inproceedings{chen2022knowledge,
  title={Knowledge distillation with the reused teacher classifier},
  author={Chen, Defang and Mei, Jian-Ping and Zhang, Hailin and Wang, Can and Feng, Yan and Chen, Chun},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  year={2022}
}


@article{kim2021comparing,
  title={Comparing kullback-leibler divergence and mean squared error loss in knowledge distillation},
  author={Kim, Taehyeon and Oh, Jaehoon and Kim, NakYil and Cho, Sangwook and Yun, Se-Young},
  journal={arXiv preprint arXiv:2105.08919},
  year={2021}
}

@article{yang2023knowledge,
  title={From Knowledge Distillation to Self-Knowledge Distillation: A Unified Approach with Normalized Loss and Customized Soft Labels},
  author={Yang, Zhendong and Zeng, Ailing and Li, Zhe and Zhang, Tianke and Yuan, Chun and Li, Yu},
  journal={arXiv preprint arXiv:2303.13005},
  year={2023}
}

@article{yang2022vitkd,
  title={ViTKD: Practical Guidelines for ViT feature knowledge distillation},
  author={Yang, Zhendong and Li, Zhe and Zeng, Ailing and Li, Zexian and Yuan, Chun and Li, Yu},
  journal={arXiv preprint arXiv:2209.02432},
  year={2022}
}

@article{amara2022bd,
  title={BD-KD: Balancing the Divergences for Online Knowledge Distillation},
  author={Amara, Ibtihel and Sepahvand, Nazanin and Meyer, Brett H and Gross, Warren J and Clark, James J},
  journal={arXiv preprint arXiv:2212.12965},
  year={2022}
}

@article{zhang2023survey,
  title={A survey on learning to reject},
  author={Zhang, Xu-Yao and Xie, Guo-Sen and Li, Xiuli and Mei, Tao and Liu, Cheng-Lin},
  journal={Proceedings of the IEEE},
  volume={111},
  number={2},
  pages={185--215},
  year={2023},
  publisher={IEEE}
}

@article{xu2022survey,
  title={A survey on dynamic neural networks for natural language processing},
  author={Xu, Canwen and McAuley, Julian},
  journal={arXiv preprint arXiv:2202.07101},
  year={2022}
}

@inproceedings{ding2020revisiting,
  title={Revisiting the evaluation of uncertainty estimation and its application to explore model complexity-uncertainty trade-off},
  author={Ding, Yukun and Liu, Jinglan and Xiong, Jinjun and Shi, Yiyu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  pages={4--5},
  year={2020}
}